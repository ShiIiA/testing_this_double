# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rRSegdTULlczjwu11IQzYDFAIJmlE8TF
"""

import os
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference
import shap
from scipy.stats import chi2_contingency
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
from torchvision.models import DenseNet121_Weights
import plotly.express as px
import openai

# ========== PAGE CONFIGURATION & UX ENHANCEMENTS ==========
st.set_page_config(
    page_title="üêç Snakelets - AI Bias Analyzer üêç",
    page_icon="üêç",
    layout="wide",
    initial_sidebar_state="expanded"
)

def set_background():
    st.markdown(
        """
        <style>
        .stApp {
            background: linear-gradient(to bottom right, #2C3E50, #4CA1AF);
            color: white;
        }
        .stTitle {
            font-size: 36px !important;
            font-weight: bold;
            color: white !important;
        }
        </style>
        """,
        unsafe_allow_html=True
    )

def set_gradient_progress_bar():
    st.markdown(
        """
        <style>
        div[data-testid="stProgressBar"] > div[role="progressbar"] > div {
            background: linear-gradient(to right, #4facfe, #00f2fe);
        }
        </style>
        """,
        unsafe_allow_html=True
    )

set_background()
set_gradient_progress_bar()

# ========== GLOBAL SESSION STATE ==========
if "df" not in st.session_state:
    st.session_state.df = None
if "df_results" not in st.session_state:
    st.session_state.df_results = pd.DataFrame(columns=["Image_ID", "Gender", "Prediction", "Probability"])

# ========== OPENAI API CONFIGURATION ==========
openai.api_key = st.secrets["OPENAI_API_KEY"]

def openai_chatbot(user_input):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",  # or use "gpt-4" if available
        messages=[
            {"role": "system", "content": "You are a helpful assistant specialized in AI bias analysis."},
            {"role": "user", "content": user_input},
        ],
        max_tokens=150,
        temperature=0.7,
    )
    return response["choices"][0]["message"]["content"]

# ========== MODEL & HELPER FUNCTIONS ==========
@st.cache_resource(show_spinner=True)
def load_chexnet_model():
    """Loads the pre-trained DenseNet-121 (CheXNet) model."""
    model = models.densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)
    model.classifier = nn.Linear(1024, 2)  # Binary classification
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()
    return model, device

try:
    model, device = load_chexnet_model()
    st.success("‚úÖ Model Loaded Successfully!")
except Exception as e:
    st.error(f"üö® Error loading model: {e}")

def unify_gender_label(label):
    text = str(label).strip().lower()
    male_keywords = ["m", "m ", " m", " m " , "male", "man", "masculin"]
    female_keywords = ["f", "f ", " f", " f ", "female", "woman", "femme"]
    if any(kw in text for kw in male_keywords):
        return "M"
    if any(kw in text for kw in female_keywords):
        return "F"
    return "Unknown"

def unify_disease_label(label):
    text = str(label).strip().lower()
    no_disease_keywords = ["no finding", "none", "negative", "normal", "0", "false", "no disease"]
    if any(kw in text for kw in no_disease_keywords):
        return "No Disease"
    return label

@st.cache_resource(show_spinner=True)
def preprocess_image(image):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])
    return transform(image).unsqueeze(0)

# ========== MODULAR PAGE FUNCTIONS ==========

def home_page():
    st.title("üè† Home")
    st.markdown("## Importance of Gender Bias in AI")
    st.markdown(
        """
        **Why Gender Bias Matters in AI:**

        - **Ethical Concerns:** Biased data can lead to unfair treatment.
        - **Clinical Impact:** In healthcare, bias might cause misdiagnosis.
        - **Regulatory Pressure:** Fairness is becoming a legal necessity.
        - **Research Evidence:** Studies consistently show that underrepresented groups are adversely affected.

        Use the sidebar to explore data, model predictions, bias analysis, mitigation strategies, and test mitigation techniques.
        """
    )
    st.info("Navigate the app using the sidebar.")

def upload_data_page():
    st.title("üìÇ Upload Data")
    uploaded_file = st.file_uploader(
        "Upload your dataset (CSV/XLSX)",
        type=["csv", "xlsx"],
        help="Upload a CSV or Excel file containing your data."
    )
    if uploaded_file:
        try:
            if uploaded_file.name.endswith(".csv"):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            st.session_state.df = df
            st.write("**Preview of Uploaded Data:**")
            st.dataframe(df.head())
        except Exception as e:
            st.error(f"Error loading file: {e}")
    else:
        st.info("Please upload a CSV or XLSX file to continue.")

def explore_data_page():
    st.title("üìä Explore Data")
    df = st.session_state.df
    if df is not None:
        st.markdown("### Data Summary")
        st.write(df.describe(include="all"))

        # Let user select columns for more detailed analysis.
        col_select = st.multiselect(
            "Select columns for detailed analysis",
            options=df.columns.tolist(),
            help="Choose one or more columns to visualize distributions and relationships."
        )
        if col_select:
            st.markdown("#### Column Distributions")
            for col in col_select:
                fig, ax = plt.subplots()
                if pd.api.types.is_numeric_dtype(df[col]):
                    ax.hist(df[col].dropna(), bins=20, color="#4facfe", edgecolor="black")
                    ax.set_title(f"Distribution of {col}")
                else:
                    counts = df[col].value_counts()
                    ax.bar(counts.index.astype(str), counts.values, color="#00f2fe", edgecolor="black")
                    ax.set_title(f"Counts of {col}")
                    plt.xticks(rotation=45)
                st.pyplot(fig)
        else:
            st.info("Select one or more columns to visualize.")

        # Additional interactive visualization: scatter plot for numeric columns.
        numeric_cols = df.select_dtypes(include=["float", "int"]).columns.tolist()
        if len(numeric_cols) >= 2:
            st.markdown("### Interactive Scatter Plot")
            x_axis = st.selectbox(
                "Select X-axis",
                options=numeric_cols,
                help="Choose a numeric column for the x-axis."
            )
            y_axis = st.selectbox(
                "Select Y-axis",
                options=numeric_cols,
                help="Choose a numeric column for the y-axis."
            )
            fig_scatter = px.scatter(
                df, x=x_axis, y=y_axis,
                title=f"Scatter Plot: {x_axis} vs {y_axis}",
                hover_data=df.columns
            )
            st.plotly_chart(fig_scatter)
        else:
            st.info("Not enough numeric columns for a scatter plot.")
    else:
        st.info("Data not uploaded or not loaded correctly. Please go to the Upload Data page.")

def model_prediction_page():
    st.title("ü§ñ Model Prediction")
    df = st.session_state.df
    if df is None:
        st.info("Please upload and explore data first.")
        st.stop()
    if "gender_col" not in st.session_state or "disease_col" not in st.session_state or "image_id_col" not in st.session_state:
        st.info("Please complete data preparation in Explore Data.")
        st.stop()
    uploaded_images = st.file_uploader(
        "Upload X-Ray Images",
        type=["png", "jpg", "jpeg"],
        accept_multiple_files=True,
        help="Upload one or more X-Ray images for prediction."
    )
    threshold = st.slider(
        "Decision Threshold for 'Disease' classification",
        0.0, 1.0, 0.5, 0.01,
        help="Adjust the threshold for classifying images as disease or no disease."
    )
    if uploaded_images:
        progress_bar = st.progress(0)
        total_images = len(uploaded_images)
        for i, img in enumerate(uploaded_images, start=1):
            st.write(f"Processing Image {i}/{total_images}")
            st.image(img, caption=f"Uploaded X-Ray: {img.name}", width=300)
            try:
                image = Image.open(img).convert("RGB")
                tensor_img = preprocess_image(image).to(device)
                with torch.no_grad():
                    logits = model(tensor_img)
                    probs = F.softmax(logits, dim=1)
                    disease_prob = probs[0, 1].item()
                    predicted_label = 1 if disease_prob >= threshold else 0
                new_row = {
                    "Image_ID": img.name,
                    "Gender": "Unknown",
                    "Prediction": predicted_label,
                    "Probability": disease_prob
                }
                st.session_state.df_results = pd.concat(
                    [st.session_state.df_results, pd.DataFrame([new_row])],
                    ignore_index=True
                )
                st.success(f"Prediction: {'Disease Detected' if predicted_label == 1 else 'No Disease'} | Prob: {disease_prob:.2%}")
            except Exception as e:
                st.error(f"Error making prediction: {e}")
            progress_bar.progress(int((i / total_images) * 100))
    else:
        st.info("Upload one or more images to generate predictions.")

def gender_bias_analysis_page():
    st.title("‚öñÔ∏è Gender Bias Analysis")
    df = st.session_state.df
    df_results = st.session_state.df_results
    if df is None or df_results.empty:
        st.info("Please complete data upload and model prediction first.")
        st.stop()
    gender_col = st.session_state.gender_col
    disease_col = st.session_state.disease_col
    image_id_col = st.session_state.image_id_col
    # Merge gender information from the original data
    if "Unknown" in df_results["Gender"].values:
        df_merged_gender = pd.merge(
            df_results,
            df[[image_id_col, gender_col]],
            how="left",
            left_on="Image_ID",
            right_on=image_id_col
        )
        df_merged_gender["Gender"] = df_merged_gender[gender_col].fillna("Unknown")
        st.session_state.df_results = df_merged_gender[["Image_ID", "Gender", "Prediction", "Probability"]]
    df_results = st.session_state.df_results
    total_female = df_results[df_results["Gender"] == "F"].shape[0]
    total_male = df_results[df_results["Gender"] == "M"].shape[0]
    female_disease = df_results[(df_results["Gender"] == "F") & (df_results["Prediction"] == 1)].shape[0]
    male_disease = df_results[(df_results["Gender"] == "M") & (df_results["Prediction"] == 1)].shape[0]
    female_rate = female_disease / total_female if total_female > 0 else 0
    male_rate = male_disease / total_male if total_male > 0 else 0
    st.write(f"**Female Detection Rate:** {female_rate:.2%}  (F: {total_female} images)")
    st.write(f"**Male Detection Rate:** {male_rate:.2%}  (M: {total_male} images)")
    bias_difference = abs(female_rate - male_rate)
    st.write(f"**Bias Difference (Female vs. Male):** {bias_difference:.4f}")
    if bias_difference > 0.1:
        st.warning("Significant bias detected toward one gender! Consider mitigating steps.")
    else:
        st.success("Bias difference is within acceptable limits.")

def bias_mitigation_simulation_page():
    st.title("üõ†Ô∏è Bias Mitigation & Simulation")
    st.markdown("### Advanced Fairness Analysis")
    df = st.session_state.df
    df_results = st.session_state.df_results
    if df is None or df_results.empty:
        st.info("Please complete data upload and model prediction first.")
        st.stop()
    gender_col = st.session_state.gender_col
    disease_col = st.session_state.disease_col
    image_id_col = st.session_state.image_id_col
    advanced_fairness = st.checkbox(
        "Use advanced fairness approach with predictions",
        help="Enable advanced fairness metrics computation."
    )
    if advanced_fairness:
        if df is not None and image_id_col in df.columns:
            df_merged = pd.merge(
                df,
                df_results,
                how="inner",
                left_on=image_id_col,
                right_on="Image_ID"
            )
            adv_target_col = st.selectbox(
                "Select Ground-Truth Disease Column (Advanced):",
                df.columns.tolist(),
                help="Choose the ground truth column for disease labels."
            )
            adv_sensitive_col = st.selectbox(
                "Select Sensitive Attribute (Advanced):",
                df.columns.tolist(),
                help="Choose the sensitive attribute (e.g., Gender)."
            )
            if adv_target_col and adv_sensitive_col:
                try:
                    y_true = df_merged[adv_target_col]
                    y_pred = df_merged["Prediction"]
                    sensitive = df_merged[adv_sensitive_col]
                    d_parity = demographic_parity_difference(y_true, y_pred, sensitive_features=sensitive)
                    eod = equalized_odds_difference(y_true, y_pred, sensitive_features=sensitive)
                    st.write(f"**Demographic Parity Difference:** {d_parity:.4f}")
                    st.write(f"**Equalized Odds Difference:** {eod:.4f}")
                    acc = accuracy_score(y_true, y_pred)
                    prec = precision_score(y_true, y_pred, zero_division=0)
                    rec = recall_score(y_true, y_pred, zero_division=0)
                    st.write(f"**Accuracy:** {acc:.2%}")
                    st.write(f"**Precision:** {prec:.2%}")
                    st.write(f"**Recall:** {rec:.2%}")
                    cm = confusion_matrix(y_true, y_pred)
                    fig_cm, ax_cm = plt.subplots()
                    cax = ax_cm.matshow(cm, cmap=plt.cm.Blues)
                    fig_cm.colorbar(cax)
                    for (i, j), val in np.ndenumerate(cm):
                        ax_cm.text(j, i, f'{val}', va='center', ha='center')
                    ax_cm.set_xticks(np.arange(2))
                    ax_cm.set_yticks(np.arange(2))
                    ax_cm.set_xticklabels(["No Disease", "Disease"])
                    ax_cm.set_yticklabels(["No Disease", "Disease"])
                    ax_cm.set_xlabel("Predicted")
                    ax_cm.set_ylabel("True")
                    ax_cm.set_title("Confusion Matrix")
                    st.pyplot(fig_cm)
                except Exception as e:
                    st.error(f"Error computing advanced metrics: {e}")
    st.markdown("---")
    st.markdown("### Possible Bias Mitigation Approaches")
    st.markdown(
        """
        **1. Resampling/Upweighting**
        Adjust training data distribution to reduce bias.

        **2. Threshold Adjustment**
        Use subgroup-specific thresholds to equalize metrics.

        **3. Reweighing**
        Weight samples to promote fair outcomes.

        **4. Adversarial Debiasing**
        Incorporate adversarial training to remove sensitive cues.

        **5. Post-Processing**
        Calibrate predictions after inference to correct bias.
        """
    )
    st.success("Analysis & mitigation recommendations complete!")

def gender_bias_testing_page():
    st.title("üß™ Gender Bias Testing")
    st.markdown("### Test Bias Mitigation via Threshold Adjustment")
    df_results = st.session_state.df_results
    if df_results.empty:
        st.info("No prediction data available. Please complete model prediction first.")
        st.stop()
    # Allow user to set separate thresholds for male and female
    threshold_m = st.slider(
        "Threshold for Male", 0.0, 1.0, 0.5, 0.01,
        help="Set the decision threshold for male predictions."
    )
    threshold_f = st.slider(
        "Threshold for Female", 0.0, 1.0, 0.5, 0.01,
        help="Set the decision threshold for female predictions."
    )
    # Create adjusted predictions based on the new thresholds
    df_new = df_results.copy()
    def adjust_prediction(row):
        if row["Gender"] == "M":
            return 1 if row["Probability"] >= threshold_m else 0
        elif row["Gender"] == "F":
            return 1 if row["Probability"] >= threshold_f else 0
        else:
            return row["Prediction"]
    df_new["Adjusted_Prediction"] = df_new.apply(adjust_prediction, axis=1)
    total_female = df_new[df_new["Gender"] == "F"].shape[0]
    total_male = df_new[df_new["Gender"] == "M"].shape[0]
    female_disease = df_new[(df_new["Gender"] == "F") & (df_new["Adjusted_Prediction"] == 1)].shape[0]
    male_disease = df_new[(df_new["Gender"] == "M") & (df_new["Adjusted_Prediction"] == 1)].shape[0]
    female_rate = female_disease / total_female if total_female > 0 else 0
    male_rate = male_disease / total_male if total_male > 0 else 0
    st.write(f"**Adjusted Female Detection Rate:** {female_rate:.2%}  (F: {total_female} images)")
    st.write(f"**Adjusted Male Detection Rate:** {male_rate:.2%}  (M: {total_male} images)")
    bias_difference = abs(female_rate - male_rate)
    st.write(f"**Adjusted Bias Difference (Female vs. Male):** {bias_difference:.4f}")
    if bias_difference > 0.1:
        st.warning("Significant bias detected even after threshold adjustment.")
    else:
        st.success("Bias difference is within acceptable limits after adjustment.")
    st.markdown("#### Adjusted Predictions Preview")
    st.dataframe(df_new.head())

def importance_gender_bias_page():
    st.title("üìö The Importance of Gender Bias")
    st.markdown(
        """
        **Addressing Gender Bias in AI is Crucial:**

        - **Ethical Imperative:** Fair treatment in AI is a moral obligation.
        - **Clinical Impact:** Biased models can lead to misdiagnosis or suboptimal care.
        - **Regulatory Requirements:** Fairness is increasingly mandated.
        - **Research Evidence:** Underrepresentation leads to poorer outcomes for affected groups.

        **Key References:**
        - Mehrabi et al. (2021). *A Survey on Bias and Fairness in Machine Learning.*
        - Obermeyer et al. (2019). *Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.*
        - Larrazabal et al. (2020). *Gender Imbalance in Medical Imaging Datasets Produces Biased AI Algorithms.*
        """
    )

def about_chexnet_model_page():
    st.title("üß† About CheXNet Model")
    st.markdown(
        """
        **CheXNet** is a deep learning model based on **DenseNet-121**.

        - **Pre-trained Dataset:** ChestX-ray14
        - **Architecture:** Convolutional Neural Network (CNN)
        - **Performance:** Comparable to radiologists in detecting pneumonia
        - **Pre-trained Weights:** Available online
        - **Technical Paper:** [CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning](https://arxiv.org/abs/1711.05225)
        """
    )

def meet_the_team_page():
    st.title("üë• Meet the Team")
    team_members = [
        {"name": "Yuying", "role": "Lead Data Scientist", "image": "to come"},
        {"name": "Siwen", "role": "ML Engineer", "image": "to come"},
        {"name": "Zhi", "role": "UX Designer", "image": "to come"},
        {"name": "Maude", "role": "AI Ethics & Fairness Specialist", "image": "to come"}
    ]
    cols = st.columns(len(team_members))
    for i, member in enumerate(team_members):
        with cols[i]:
            st.image(member["image"], width=150)
            st.write(f"**{member['name']}**")
            st.write(f"*{member['role']}*")

def chatbot_page():
    st.title("üí¨ AI Chatbot Helper")
    st.markdown("Ask questions about the analysis below. The chatbot will help you understand key aspects such as bias, model behavior, thresholds, and mitigation techniques using OpenAI.")
    if "chat_history" not in st.session_state:
         st.session_state.chat_history = []
    with st.form("chat_form", clear_on_submit=True):
         user_message = st.text_input(
             "Your question:",
             key="chat_message",
             help="Type a question (e.g., 'How does threshold adjustment mitigate bias?')"
         )
         submitted = st.form_submit_button("Send")
         if submitted and user_message:
             response = openai_chatbot(user_message)
             st.session_state.chat_history.append(("You", user_message))
             st.session_state.chat_history.append(("Chatbot", response))
    st.markdown("### Conversation")
    for speaker, message in st.session_state.chat_history:
         st.markdown(f"**{speaker}:** {message}")

# ========== SIDEBAR NAVIGATION ==========
page_options = [
    "üè† Home",
    "üìÇ Upload Data",
    "üìä Explore Data",
    "ü§ñ Model Prediction",
    "‚öñÔ∏è Gender Bias Analysis",
    "üõ†Ô∏è Bias Mitigation & Simulation",
    "üß™ Gender Bias Testing",
    "üí¨ AI Chatbot",
    "üìö The Importance of Gender Bias",
    "üß† About CheXNet Model",
    "üë• Meet the Team"
]

selected_page = st.sidebar.radio(
    "Navigate to",
    page_options,
    help="Select a section to explore."
)

# ========== PAGE RENDERING ==========
if selected_page == "üè† Home":
    home_page()
elif selected_page == "üìÇ Upload Data":
    upload_data_page()
elif selected_page == "üìä Explore Data":
    explore_data_page()
elif selected_page == "ü§ñ Model Prediction":
    model_prediction_page()
elif selected_page == "‚öñÔ∏è Gender Bias Analysis":
    gender_bias_analysis_page()
elif selected_page == "üõ†Ô∏è Bias Mitigation & Simulation":
    bias_mitigation_simulation_page()
elif selected_page == "üß™ Gender Bias Testing":
    gender_bias_testing_page()
elif selected_page == "üí¨ AI Chatbot":
    chatbot_page()
elif selected_page == "üìö The Importance of Gender Bias":
    importance_gender_bias_page()
elif selected_page == "üß† About CheXNet Model":
    about_chexnet_model_page()
elif selected_page == "üë• Meet the Team":
    meet_the_team_page()