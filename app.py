# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rRSegdTULlczjwu11IQzYDFAIJmlE8TF
"""

import os
import io
import logging
import tempfile
import warnings  # to suppress warnings
import subprocess
import streamlit as st
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
from transformers import AutoModelForCausalLM, AutoTokenizer

# ------------------------- Logging Configuration -------------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ------------------------- Page Configuration -------------------------
st.set_page_config(
    page_title="Gender Bias in Radiology",
    page_icon="üêç",
    layout="wide"
)

# ------------------------- THEME & STYLE FUNCTIONS -------------------------
def set_background():
    st.markdown(
        \"\"\"
        <style>
        .stApp {
            background: linear-gradient(to bottom right, #ffffff, #e6f7ff);
            color: #000;
        }
        .stTitle {
            font-size: 36px !important;
            font-weight: bold;
            color: #000 !important;
        }
        </style>
        \"\"\", unsafe_allow_html=True
    )
set_background()

# ------------------------- MODEL & HELPER FUNCTIONS -------------------------
@st.cache_resource(show_spinner=True)
def load_chexnet_model():
    model = models.densenet121(weights="IMAGENET1K_V1")
    model.classifier = nn.Linear(1024, 2)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.eval()
    return model, device

@st.cache_resource(show_spinner=True)
def load_chexagent_model():
    model_name = "StanfordAIMI/CheXagent-2-3b"
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_name, device_map="cpu", trust_remote_code=True)
    model = model.to(torch.bfloat16)
    model.eval()
    return model, tokenizer

try:
    chexnet_model, device = load_chexnet_model()
    chexagent_model, chexagent_tokenizer = load_chexagent_model()
    st.sidebar.success("‚úÖ Models Loaded Successfully!")
except Exception as e:
    logging.error("Error loading models", exc_info=True)
    st.sidebar.error(f"üö® Error loading models: {e}")

# ------------------------- IMAGE PROCESSING & PREDICTION -------------------------
def preprocess_image(image):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])
    return transform(image).unsqueeze(0)

def predict_chexnet(image):
    tensor_img = preprocess_image(image)
    tensor_img = tensor_img.to(device)
    with torch.no_grad():
        logits = chexnet_model(tensor_img)
        probs = F.softmax(logits, dim=1)
        disease_prob = probs[0, 1].item()
        predicted_binary = 1 if disease_prob >= 0.5 else 0
    pred_disease = "Disease Detected" if predicted_binary == 1 else "No Disease"
    return pred_disease, disease_prob

def predict_chexagent(image, prompt="Analyze the chest X‚Äëray image and return the disease name."):
    query = chexagent_tokenizer.from_list_format([{'image': image}, {'text': prompt}])
    conv = [
        {"from": "system", "value": "You are a helpful assistant."},
        {"from": "human", "value": query}
    ]
    input_ids = chexagent_tokenizer.apply_chat_template(conv, add_generation_prompt=True, return_tensors="pt")
    output = chexagent_model.generate(
        input_ids.to("cpu"),
        do_sample=False,
        num_beams=1,
        temperature=1.0,
        top_p=1.0,
        use_cache=True,
        max_new_tokens=128
    )[0]
    response = chexagent_tokenizer.decode(output[input_ids.size(1):-1])
    return response

# ------------------------- COMPARATIVE ANALYSIS PAGE -------------------------
def comparative_analysis_page():
    st.title("üî¨ Compare CheXNet & CheXagent Predictions")
    uploaded_images = st.file_uploader("Upload Chest X-ray Images", type=["png", "jpg", "jpeg"], accept_multiple_files=True)

    if uploaded_images:
        with st.spinner("Processing images..."):
            for img in uploaded_images:
                st.image(img, caption=f"Uploaded: {img.name}", width=300)
                image = Image.open(img).convert("RGB")

                # Predictions from both models
                chexnet_pred, chexnet_prob = predict_chexnet(image)
                chexagent_pred = predict_chexagent(image)

                # Display results
                st.markdown("### **Results:**")
                col1, col2 = st.columns(2)
                with col1:
                    st.subheader("üß† CheXNet Prediction")
                    st.write(f"Prediction: {chexnet_pred}")
                    st.write(f"Probability: {chexnet_prob:.2%}")
                with col2:
                    st.subheader("ü§ñ CheXagent Prediction")
                    st.write(f"Response: {chexagent_pred}")

# ------------------------- SIDEBAR NAVIGATION -------------------------
page_options = [
    "üè† Home",
    "üî¨ Compare Models",
    "üìö The Importance of Gender Bias",
    "üë• Meet the Team"
]

selected_page = st.sidebar.radio("Navigate to", page_options, help="Select a section to explore.")

# ------------------------- PAGE RENDERING -------------------------
if selected_page == "üè† Home":
    st.title("üè† Home")
    st.markdown("Welcome to the Gender Bias in Radiology app!")
elif selected_page == "üî¨ Compare Models":
    comparative_analysis_page()
elif selected_page == "üìö The Importance of Gender Bias":
    st.title("üìö The Importance of Gender Bias")
    st.markdown("Learn why gender bias in AI is a crucial issue.")
elif selected_page == "üë• Meet the Team":
    st.title("üë• Meet the Team")
    st.markdown("This project is brought to you by an amazing team of data scientists!")


# Save the fixed and optimized app script
with open(fixed_app_path, "w") as f:
    f.write(fixed_app_code)

# Provide the fixed file to the user
fixed_app_path