# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rRSegdTULlczjwu11IQzYDFAIJmlE8TF
"""

import os
import logging
import warnings
import streamlit as st
import pandas as pd
import numpy as np
import altair as alt
import plotly.express as px
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from PIL import Image
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from collections import Counter
import re

# Install required packages (uncomment for first run)
# import subprocess
# subprocess.run(["pip", "install", "torchxrayvision"], check=True)
import torchxrayvision as xrv

# Suppress warnings
warnings.filterwarnings("ignore")

# ------------------------- LOGGING CONFIGURATION -------------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ------------------------- PAGE CONFIGURATION -------------------------
st.set_page_config(
    page_title="Gender Bias in Radiology",
    page_icon="🩺",
    layout="wide"
)

# ------------------------- THEME & STYLE FUNCTIONS -------------------------
def set_background():
    style = """
    <style>
    .stApp {
        background: linear-gradient(to bottom right, #ffffff, #e6f7ff);
        color: #000;
    }
    .stTitle {
        font-size: 36px !important;
        font-weight: bold;
        color: #000 !important;
    }
    </style>
    """
    st.markdown(style, unsafe_allow_html=True)

set_background()

def set_gradient_progress_bar():
    st.markdown(
        """
        <style>
        div[data-testid="stProgressBar"] > div[role="progressbar"] > div {
            background: linear-gradient(to right, #4facfe, #00f2fe);
        }
        </style>
        """,
        unsafe_allow_html=True
    )

set_gradient_progress_bar()

# ------------------------- MODEL CONFIGURATIONS -------------------------
MODELS = {
    "DenseNet121": {
        "source": "pytorch_hub",
        "name": "densenet121",
        "pretrained": True,
        "classes": 14,
        "description": "DenseNet architecture pre-trained on ImageNet and adapted for chest X-ray analysis"
    },
    "ResNet50": {
        "source": "pytorch_hub",
        "name": "resnet50",
        "pretrained": True,
        "classes": 14,
        "description": "Residual network architecture pre-trained on ImageNet and adapted for chest X-ray analysis"
    },
    "CheXpert": {
        "source": "torchxrayvision",
        "name": "densenet121-res224-chex",
        "classes": 14,
        "description": "DenseNet121 architecture specifically trained on the CheXpert dataset from Stanford"
    },
    "MIMIC-CXR": {
        "source": "torchxrayvision",
        "name": "densenet121-res224-mimic_nb",
        "classes": 14,
        "description": "DenseNet121 architecture trained on the MIMIC-CXR dataset from MIT"
    }
}

# ------------------------- GLOBAL SESSION STATE -------------------------
if "df" not in st.session_state:
    st.session_state.df = None
if "disease_col" not in st.session_state:
    st.session_state.disease_col = None
if "gender_col" not in st.session_state:
    st.session_state.gender_col = None
if "image_id_col" not in st.session_state:
    st.session_state.image_id_col = None
if "age_col" not in st.session_state:
    st.session_state.age_col = None
if "df_results" not in st.session_state:
    st.session_state.df_results = pd.DataFrame(columns=["Image_ID", "Gender", "Actual", "Prediction", "Probability", "Model"])
if "disease_classes" not in st.session_state:
    st.session_state.disease_classes = []
if "models_loaded" not in st.session_state:
    st.session_state.models_loaded = {}
if "device" not in st.session_state:
    st.session_state.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ------------------------- MODEL & HELPER FUNCTIONS -------------------------
def load_model(model_name):
    """Load the specified model from reliable sources"""
    if model_name in st.session_state.models_loaded:
        return st.session_state.models_loaded[model_name]

    try:
        model_info = MODELS[model_name]

        with st.spinner(f"Loading {model_name} model..."):
            # Load pre-trained model from PyTorch Hub
            if model_info["source"] == "pytorch_hub":
                base_model = torch.hub.load('pytorch/vision:v0.10.0',
                                           model_info["name"],
                                           pretrained=model_info["pretrained"])

                # Modify the classifier for chest X-ray tasks
                if model_name == "DenseNet121":
                    num_ftrs = base_model.classifier.in_features
                    base_model.classifier = nn.Linear(num_ftrs, model_info["classes"])
                elif model_name == "ResNet50":
                    num_ftrs = base_model.fc.in_features
                    base_model.fc = nn.Linear(num_ftrs, model_info["classes"])

                # Move model to appropriate device
                model = base_model.to(st.session_state.device)
                model.eval()

                st.session_state.models_loaded[model_name] = model
                st.success(f"✅ {model_name} loaded successfully!")
                return model

            # Load pre-trained model from TorchXRayVision
            elif model_info["source"] == "torchxrayvision":
                model = xrv.models.DenseNet(weights=model_info["name"])
                model = model.to(st.session_state.device)
                model.eval()

                st.session_state.models_loaded[model_name] = model
                st.success(f"✅ {model_name} loaded successfully!")
                return model

    except Exception as e:
        logging.error(f"Error loading {model_name} model", exc_info=True)
        st.error(f"🚨 Error loading {model_name} model: {e}")
        return None

def unify_gender_label(label):
    """Convert various gender labels to standardized format"""
    text = str(label).strip().lower()
    male_keywords = ["m", "male", "man", "masculin"]
    female_keywords = ["f", "female", "woman", "femme"]

    if any(kw in text for kw in male_keywords):
        return "M"
    if any(kw in text for kw in female_keywords):
        return "F"
    return "Unknown"

def unify_disease_label(label):
    """Convert various disease labels to standardized format"""
    text = str(label).strip().lower()
    no_disease_keywords = ["no finding", "none", "negative", "normal", "0", "false", "no disease"]

    if any(kw in text for kw in no_disease_keywords):
        return "No Disease"
    return text

def preprocess_image(image, model_name):
    """Preprocess image for model input based on model type"""
    # Convert to grayscale first for medical imaging models
    if image.mode != 'L':  # If not already grayscale
        image = image.convert('L')

    # Different preprocessing depending on model source
    if model_name in MODELS and MODELS[model_name]["source"] == "torchxrayvision":
        # Use TorchXRayVision's preprocessing
        # Convert PIL image to numpy array and ensure proper scaling
        img_np = np.array(image).astype(np.float32)
        # Normalize to 0-1 range before passing to xrv.datasets.normalize
        if img_np.max() > 0:
            img_np = img_np / 255.0
        # Now use xrv's normalize function with 224 as target size
        img = xrv.datasets.normalize(img_np, 224)
        tensor_img = torch.from_numpy(img).unsqueeze(0)
    else:
        # Standard preprocessing for PyTorch models
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485], std=[0.229])  # Single channel normalization
        ])
        tensor_img = transform(image).unsqueeze(0)

    return tensor_img

def predict_disease(image, model_name, threshold=0.5):
    """Predict disease from image using specified model"""
    if model_name not in st.session_state.models_loaded:
        st.error(f"Model {model_name} not loaded. Please load it first.")
        return None, None

    model = st.session_state.models_loaded[model_name]
    tensor_img = preprocess_image(image, model_name).to(st.session_state.device)

    with torch.no_grad():
        if MODELS[model_name]["source"] == "torchxrayvision":
            # TorchXRayVision models have different outputs
            outputs = model(tensor_img)
            probs = torch.sigmoid(outputs)

            # Use predefined label names
            label_names = xrv.datasets.default_pathologies
        else:
            # Standard models
            outputs = model(tensor_img)
            probs = torch.sigmoid(outputs)

            # Use custom labels if available, otherwise generic ones
            if st.session_state.disease_classes:
                label_names = st.session_state.disease_classes
            else:
                label_names = [f"Disease_{i}" for i in range(probs.shape[1])]

        # Get highest probability
        max_prob, max_idx = torch.max(probs[0], dim=0)
        max_prob_val = max_prob.item()

        if max_prob_val >= threshold:
            predicted_label = label_names[max_idx]
        else:
            predicted_label = "No Disease"

        return predicted_label, max_prob_val

def compute_fairness_metrics(df, protected_attribute, target, prediction):
    """Compute fairness metrics based on predictions"""
    try:
        # Group dataframe by protected attribute
        groups = df[protected_attribute].unique()
        metrics = {}

        for group in groups:
            group_df = df[df[protected_attribute] == group]
            if len(group_df) == 0:
                continue

            y_true = group_df[target]
            y_pred = group_df[prediction]

            metrics[group] = {
                'size': len(group_df),
                'positive_rate': y_pred.mean(),
                'accuracy': accuracy_score(y_true, y_pred),
                'precision': precision_score(y_true, y_pred, zero_division=0),
                'recall': precision_score(y_true, y_pred, zero_division=0),
                'f1': f1_score(y_true, y_pred, zero_division=0)
            }

            try:
                metrics[group]['auc'] = roc_auc_score(y_true, y_pred)
            except:
                metrics[group]['auc'] = float('nan')

        # Calculate disparities
        if len(metrics) >= 2:
            groups_list = list(metrics.keys())
            disparities = {
                'positive_rate_disparity': abs(metrics[groups_list[0]]['positive_rate'] - metrics[groups_list[1]]['positive_rate']),
                'accuracy_disparity': abs(metrics[groups_list[0]]['accuracy'] - metrics[groups_list[1]]['accuracy']),
                'precision_disparity': abs(metrics[groups_list[0]]['precision'] - metrics[groups_list[1]]['precision']),
                'recall_disparity': abs(metrics[groups_list[0]]['recall'] - metrics[groups_list[1]]['recall']),
                'f1_disparity': abs(metrics[groups_list[0]]['f1'] - metrics[groups_list[1]]['f1']),
            }

            try:
                disparities['auc_disparity'] = abs(metrics[groups_list[0]]['auc'] - metrics[groups_list[1]]['auc'])
            except:
                disparities['auc_disparity'] = float('nan')

            metrics['disparities'] = disparities

        return metrics
    except Exception as e:
        logging.error("Error computing fairness metrics", exc_info=True)
        st.error(f"Error computing fairness metrics: {e}")
        return {}

def apply_bias_mitigation(df, protected_attribute, prediction_col, probability_col, method="threshold_adjustment"):
    """Apply bias mitigation techniques to predictions"""
    try:
        df_mitigated = df.copy()
        groups = df[protected_attribute].unique()

        if method == "threshold_adjustment":
            # Find optimal thresholds for each group
            thresholds = {}

            for group in groups:
                group_df = df[df[protected_attribute] == group]
                if len(group_df) == 0:
                    continue

                # Get the current positive rate
                current_rate = group_df[prediction_col].mean()
                thresholds[group] = 0.5  # Default threshold

                # We'll adjust threshold based on overall positive rate
                # This is a simple approach - more sophisticated methods can be implemented
                overall_rate = df[prediction_col].mean()

                # If this group has higher positive rate than overall, increase threshold
                if current_rate > overall_rate:
                    thresholds[group] = 0.5 + (current_rate - overall_rate) / 2

                # If this group has lower positive rate than overall, decrease threshold
                elif current_rate < overall_rate:
                    thresholds[group] = 0.5 - (overall_rate - current_rate) / 2

            # Apply the new thresholds
            for group, threshold in thresholds.items():
                group_mask = df_mitigated[protected_attribute] == group
                df_mitigated.loc[group_mask, "Mitigated_Prediction"] = (
                    df_mitigated.loc[group_mask, probability_col] >= threshold
                ).astype(int)

        elif method == "reweighing":
            # Reweighing assigns weights to training instances to ensure fairness
            # This is a simplified version for demonstration
            weights = {}
            total_count = len(df)

            for group in groups:
                group_df = df[df[protected_attribute] == group]
                if len(group_df) == 0:
                    continue

                group_size = len(group_df)
                group_positive = group_df[prediction_col].sum()

                # Calculate weights inversely proportional to positive rate
                if group_positive > 0:
                    weights[group] = total_count / (2 * group_size * (group_positive / group_size))
                else:
                    weights[group] = 1.0

            # Apply weights to adjust predictions
            # This is simplified - in practice, you would retrain the model with these weights
            df_mitigated["Mitigated_Prediction"] = df_mitigated[prediction_col]

        else:
            # Default to copying original predictions
            df_mitigated["Mitigated_Prediction"] = df_mitigated[prediction_col]

        return df_mitigated

    except Exception as e:
        logging.error(f"Error applying bias mitigation: {e}", exc_info=True)
        return df

# ------------------------- PAGE FUNCTIONS -------------------------
def home_page():
    st.title("🏠 Home")
    st.markdown("## Addressing Gender Bias in AI-Driven Radiology")
    st.markdown(
        """
        **Why Gender Bias Matters in Medical AI:**

        - **Ethical concerns:** Unfair treatment may result from biased predictions
        - **Clinical impact:** Risk of misdiagnosis for underrepresented groups
        - **Regulatory pressure:** Fairness requirements in AI medical applications
        - **Research evidence:** Underrepresentation in training data leads to poorer outcomes

        This application helps you:
        1. Upload and analyze medical imaging datasets
        2. Detect diseases using state-of-the-art AI models
        3. Assess gender bias in model predictions
        4. Apply bias mitigation techniques
        5. Compare performance before and after bias mitigation
        """
    )
    st.info("Use the sidebar to navigate through the app.")

    # Display model information
    st.markdown("## Available Models")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### DenseNet121")
        st.markdown("Pre-trained ImageNet model adapted for chest X-ray diagnosis.")
        if st.button("Load DenseNet121", key="load_densenet"):
            load_model("DenseNet121")

        st.markdown("### ResNet50")
        st.markdown("Powerful residual network architecture for chest X-ray analysis.")
        if st.button("Load ResNet50", key="load_resnet"):
            load_model("ResNet50")

    with col2:
        st.markdown("### CheXpert")
        st.markdown("Stanford CheXpert model trained on 224,000+ chest X-rays.")
        if st.button("Load CheXpert", key="load_chexpert"):
            load_model("CheXpert")

        st.markdown("### MIMIC-CXR")
        st.markdown("MIT model trained on MIMIC-CXR dataset with diverse demographics.")
        if st.button("Load MIMIC-CXR", key="load_mimic"):
            load_model("MIMIC-CXR")

    st.markdown("---")
    st.markdown("### Acknowledgments")
    st.markdown(
        """
        **Thank you to:**
        - The TorchXRayVision team for providing open-source pre-trained models
        - The WiDS Datathon organizers for their support
        - Stanford ML Group for the CheXpert dataset
        - MIT for the MIMIC-CXR dataset
        """
    )

def upload_data_page():
    st.title("📂 Upload Data")
    uploaded_file = st.file_uploader("Upload your dataset (CSV/XLSX)", type=["csv", "xlsx"],
                                     help="Upload a CSV or Excel file containing your data.")
    if uploaded_file:
        try:
            df = pd.read_csv(uploaded_file) if uploaded_file.name.endswith(".csv") else pd.read_excel(uploaded_file)
            # Remove duplicate columns if any
            df = df.loc[:, ~df.columns.duplicated()]

            st.session_state.df = df
            st.write("**Preview of Uploaded Data:**")
            st.dataframe(df.head())

            # Let user select important columns
            st.markdown("### Select Key Columns")
            st.markdown("Please identify the columns that contain the following information:")

            # Columns for identification, gender, disease, and optional age
            st.session_state.image_id_col = st.selectbox("Image ID Column:", df.columns.tolist(),
                                                        help="Column containing unique identifiers for each image")

            st.session_state.gender_col = st.selectbox("Gender Column:", df.columns.tolist(),
                                                      help="Column containing gender information (M/F)")

            st.session_state.disease_col = st.selectbox("Disease Column:", df.columns.tolist(),
                                                       help="Column containing disease information")

            st.session_state.age_col = st.selectbox("Age Column (optional):", [None] + df.columns.tolist(),
                                                   help="Optional column containing patient age")

            # Extract and standardize gender values
            df[st.session_state.gender_col] = df[st.session_state.gender_col].apply(unify_gender_label)

            # Extract and standardize disease values
            df[st.session_state.disease_col] = df[st.session_state.disease_col].apply(unify_disease_label)

            # Extract unique disease values
            disease_classes = df[st.session_state.disease_col].dropna().unique().tolist()
            st.session_state.disease_classes = sorted(disease_classes)

            if not disease_classes:
                st.warning("⚠️ No diseases detected in the selected column.")
            else:
                st.success(f"✅ Detected {len(disease_classes)} diseases/conditions: {', '.join(st.session_state.disease_classes)}")

            # Show gender distribution
            gender_counts = df[st.session_state.gender_col].value_counts()
            st.markdown("### Gender Distribution")
            st.bar_chart(gender_counts)

            # Save processed dataframe
            st.session_state.df = df

        except Exception as e:
            logging.error("Error loading uploaded file", exc_info=True)
            st.error(f"Error loading file: {e}")
    else:
        st.info("Please upload a dataset to continue.")

def explore_data_page():
    st.title("📊 Explore Data & Prepare")
    df = st.session_state.df
    if df is not None:
        st.subheader("Select Columns for Visualization")
        selected_cols = st.multiselect("Choose one or more columns to visualize:", df.columns.tolist())
        if selected_cols:
            for col in selected_cols:
                st.markdown(f"### Visualization for **{col}**")
                if pd.api.types.is_numeric_dtype(df[col]):
                    chart = alt.Chart(df).mark_bar().encode(
                        alt.X(f"{col}:Q", bin=alt.Bin(maxbins=20), title=col),
                        alt.Y("count()", title="Count"),
                        tooltip=[col, "count()"]
                    ).properties(width=600, height=400, title=f"Histogram of {col}")
                    st.altair_chart(chart, use_container_width=True)
                else:
                    counts = df[col].value_counts().reset_index()
                    counts.columns = [col, 'Count']
                    chart = alt.Chart(counts).mark_bar().encode(
                        alt.X(f"{col}:N", sort='-y', title=col),
                        alt.Y("Count:Q", title="Count"),
                        tooltip=[col, "Count"]
                    ).properties(width=600, height=400, title=f"Bar Chart of {col}")
                    st.altair_chart(chart, use_container_width=True)
        else:
            st.info("Please select at least one column for visualization.")

        st.markdown("### Missing Values Summary")
        missing = df.isnull().sum().reset_index()
        missing.columns = ['Column', 'Missing Values']
        missing['Percentage'] = (missing['Missing Values'] / len(df) * 100).round(2)
        st.dataframe(missing.sort_values('Missing Values', ascending=False))

        # Show disease and gender relationship
        if all(col in st.session_state for col in ['disease_col', 'gender_col']):
            disease_col = st.session_state.disease_col
            gender_col = st.session_state.gender_col
            age_col = st.session_state.age_col

            st.markdown("### Disease and Gender Visualization")
            selected_disease = st.selectbox("Select Disease Category:",
                                           options=sorted(df[disease_col].dropna().unique()))

            filtered_df = df[df[disease_col] == selected_disease]
            if filtered_df.empty:
                st.info("No records found for the selected disease category.")
            else:
                # Pre-aggregate gender counts for the pie chart
                gender_counts = filtered_df[gender_col].value_counts().reset_index()
                gender_counts.columns = [gender_col, "Count"]

                pie_chart = px.pie(
                    gender_counts,
                    names=gender_col,
                    values="Count",
                    title=f"Gender Distribution for {selected_disease}",
                    color=gender_col,
                    color_discrete_map={"F": "pink", "M": "blue", "Unknown": "gray"},
                    hover_data=["Count"]
                )

                # Update trace to show percentage
                pie_chart.update_traces(texttemplate='%{percent:.1%}', textposition='inside')
                st.plotly_chart(pie_chart, use_container_width=True)

                # Show age distribution if age column is available
                if age_col:
                    try:
                        df[age_col] = pd.to_numeric(df[age_col], errors='coerce')

                        # Age distribution by gender for the selected disease
                        st.markdown(f"### Age Distribution for {selected_disease} by Gender")

                        fig = px.box(filtered_df, x=gender_col, y=age_col,
                                     title=f"Age Distribution for {selected_disease} by Gender",
                                     color=gender_col, color_discrete_map={"F": "pink", "M": "blue", "Unknown": "gray"})
                        st.plotly_chart(fig, use_container_width=True)
                    except Exception as e:
                        st.warning(f"Could not process age data: {e}")

            # Disease distribution across genders
            st.markdown("### Disease Distribution Across Genders")

            # Create pivot table
            pivot_df = pd.pivot_table(df, index=disease_col, columns=gender_col, aggfunc='size', fill_value=0)

            # Add percentages
            total_counts = pivot_df.sum(axis=1)
            for col in pivot_df.columns:
                pivot_df[f"{col}_pct"] = (pivot_df[col] / total_counts * 100).round(1)

            # Sort by total counts
            pivot_df = pivot_df.sort_values(by=total_counts.name, ascending=False)

            # Display the table
            st.dataframe(pivot_df)

            # Visualize disease distribution
            disease_counts = df[disease_col].value_counts().head(10)
            st.markdown("### Top 10 Diseases/Conditions")
            st.bar_chart(disease_counts)

        else:
            st.info("Please select disease and gender columns in the Upload Data page.")
    else:
        st.info("No data uploaded. Please use the Upload Data page.")

def model_prediction_page():
    st.title("🤖 Model Prediction")
    st.markdown("Select an AI model and upload chest X-ray images for prediction.")

    if st.session_state.df is None:
        st.warning("⚠️ Please upload and process a dataset before making predictions.")
        return

    if not st.session_state.disease_classes:
        st.warning("⚠️ No disease classes detected. Please ensure you've selected the correct disease column.")
        return

    # Check if any models are loaded
    if not st.session_state.models_loaded:
        st.warning("⚠️ No models are loaded. Please go to the Home page and load at least one model.")
        return

    # Model selection
    model_choice = st.selectbox(
        "Select AI Model:",
        list(st.session_state.models_loaded.keys()),
        help="Choose the model to use for prediction."
    )

    # Decision threshold
    threshold = st.slider(
        "Decision Threshold",
        0.0, 1.0, 0.5, 0.01,
        help="Adjust the threshold for positive disease classification."
    )

    # Upload images
    uploaded_images = st.file_uploader(
        "Upload X-ray Images",
        type=["png", "jpg", "jpeg"],
        accept_multiple_files=True,
        help="Upload one or more chest X-ray images for analysis."
    )

    if uploaded_images:
        with st.spinner("Processing images..."):
            progress_bar = st.progress(0)
            total_images = len(uploaded_images)
            results = []

            for i, img in enumerate(uploaded_images, start=1):
                col1, col2 = st.columns([1, 2])
                with col1:
                    st.write(f"Processing Image {i}/{total_images}")
                    st.image(img, caption=f"Uploaded: {img.name}", width=300)

                with col2:
                    try:
                        # Process image
                        image = Image.open(img).convert("L")  # Convert to grayscale explicitly
                        predicted_label, confidence = predict_disease(image, model_choice, threshold)

                        # Try to find the actual label and gender in the dataset
                        image_id = os.path.splitext(img.name)[0]
                        df = st.session_state.df
                        id_col = st.session_state.image_id_col

                        # Find the image in the dataset
                        matches = df[df[id_col].astype(str).str.contains(image_id, case=False, na=False)]

                        if not matches.empty:
                            # Get actual disease and gender
                            actual_disease = matches.iloc[0][st.session_state.disease_col]
                            gender = matches.iloc[0][st.session_state.gender_col]
                        else:
                            actual_disease = "Unknown"
                            gender = "Unknown"

# Add result to our tracking dataframe
                        new_row = {
                            "Image_ID": img.name,
                            "Gender": gender,
                            "Actual": actual_disease,
                            "Prediction": predicted_label,
                            "Probability": confidence,
                            "Model": model_choice
                        }
                        results.append(new_row)

                        # Display prediction results
                        st.markdown(f"**Prediction:** {predicted_label}")
                        st.markdown(f"**Confidence:** {confidence:.2%}")
                        st.markdown(f"**Actual Disease:** {actual_disease}")
                        st.markdown(f"**Gender:** {gender}")

                        # Show if prediction matches actual
                        if actual_disease != "Unknown":
                            if predicted_label == actual_disease:
                                st.success("✅ Correct prediction!")
                            else:
                                st.error("❌ Incorrect prediction")

                    except Exception as e:
                        logging.error(f"Error processing image {img.name}", exc_info=True)
                        st.error(f"❌ Error processing image: {e}")

                progress_bar.progress(int((i / total_images) * 100))

            # Update results in session state
            if results:
                new_results_df = pd.DataFrame(results)
                st.session_state.df_results = pd.concat([st.session_state.df_results, new_results_df], ignore_index=True)

                # Show summary of predictions
                st.markdown("### Prediction Summary")
                st.dataframe(new_results_df)

                # Calculate and display accuracy
                if "Actual" in new_results_df.columns and not all(new_results_df["Actual"] == "Unknown"):
                    accuracy = (new_results_df["Prediction"] == new_results_df["Actual"]).mean()
                    st.markdown(f"**Batch Accuracy:** {accuracy:.2%}")
    else:
        st.info("Upload images to generate predictions.")

def gender_bias_analysis_page():
    st.title("⚖️ Gender Bias Analysis")

    df_results = st.session_state.df_results
    if df_results.empty:
        st.info("No prediction data available yet. Please make predictions first.")
        return

    st.markdown("### Overview of Predictions by Gender")

    # Filter out rows with unknown gender
    df_gender = df_results[df_results["Gender"] != "Unknown"]

    if df_gender.empty:
        st.warning("No predictions with known gender available for analysis.")
        return

    # Count predictions by gender
    gender_counts = df_gender["Gender"].value_counts()
    st.write("**Total predictions by gender:**")

    # Create columns to display gender statistics
    col1, col2 = st.columns(2)

    with col1:
        total_F = gender_counts.get("F", 0)
        total_M = gender_counts.get("M", 0)

        st.metric("Female", f"{total_F}")
        st.metric("Male", f"{total_M}")

        # Calculate gender ratio
        if total_M > 0:
            gender_ratio = total_F / total_M
            st.write(f"**Female-to-Male Ratio:** {gender_ratio:.2f}")

    with col2:
        # Create pie chart of gender distribution
        fig = px.pie(
            values=[total_F, total_M],
            names=["Female", "Male"],
            title="Gender Distribution",
            color=["Female", "Male"],
            color_discrete_map={"Female": "pink", "Male": "blue"}
        )
        st.plotly_chart(fig, use_container_width=True)

    # Analyze disease predictions by gender
    st.markdown("### Disease Prediction Analysis by Gender")

    # Count positive predictions (predicted disease)
    pos_F = df_gender[(df_gender["Gender"] == "F") & (df_gender["Prediction"] != "No Disease")].shape[0]
    pos_M = df_gender[(df_gender["Gender"] == "M") & (df_gender["Prediction"] != "No Disease")].shape[0]

    # Calculate positive prediction rates
    rate_F = pos_F / total_F if total_F > 0 else 0
    rate_M = pos_M / total_M if total_M > 0 else 0

    col1, col2 = st.columns(2)

    with col1:
        st.write(f"**Female Disease Detection Rate:** {rate_F:.2%}")
        st.write(f"**Male Disease Detection Rate:** {rate_M:.2%}")

        # Calculate bias difference
        bias_diff = abs(rate_F - rate_M)
        st.write(f"**Absolute Bias Difference:** {bias_diff:.4f}")

        # Show bias assessment
        if bias_diff > 0.15:
            st.error("⚠️ Significant gender bias detected!")
        elif bias_diff > 0.05:
            st.warning("⚠️ Moderate gender bias detected")
        else:
            st.success("✅ Low gender bias detected")

    with col2:
        # Bar chart comparing detection rates
        comp_data = pd.DataFrame({
            "Gender": ["Female", "Male"],
            "Detection Rate": [rate_F, rate_M]
        })

        fig = px.bar(
            comp_data,
            x="Gender",
            y="Detection Rate",
            color="Gender",
            color_discrete_map={"Female": "pink", "Male": "blue"},
            title="Disease Detection Rate by Gender"
        )
        st.plotly_chart(fig, use_container_width=True)

    # Accuracy analysis by gender if we have ground truth
    if "Actual" in df_gender.columns and not all(df_gender["Actual"] == "Unknown"):
        st.markdown("### Accuracy Analysis by Gender")

        # Calculate accuracy by gender
        acc_F = df_gender[df_gender["Gender"] == "F"]["Prediction"] == df_gender[df_gender["Gender"] == "F"]["Actual"]
        acc_M = df_gender[df_gender["Gender"] == "M"]["Prediction"] == df_gender[df_gender["Gender"] == "M"]["Actual"]

        acc_F_rate = acc_F.mean() if not acc_F.empty else 0
        acc_M_rate = acc_M.mean() if not acc_M.empty else 0

        col1, col2 = st.columns(2)

        with col1:
            st.write(f"**Female Accuracy:** {acc_F_rate:.2%}")
            st.write(f"**Male Accuracy:** {acc_M_rate:.2%}")

            # Calculate accuracy disparity
            acc_diff = abs(acc_F_rate - acc_M_rate)
            st.write(f"**Accuracy Disparity:** {acc_diff:.4f}")

            # Show accuracy disparity assessment
            if acc_diff > 0.15:
                st.error("⚠️ Significant accuracy disparity!")
            elif acc_diff > 0.05:
                st.warning("⚠️ Moderate accuracy disparity")
            else:
                st.success("✅ Low accuracy disparity")

        with col2:
            # Bar chart comparing accuracy
            acc_data = pd.DataFrame({
                "Gender": ["Female", "Male"],
                "Accuracy": [acc_F_rate, acc_M_rate]
            })

            fig = px.bar(
                acc_data,
                x="Gender",
                y="Accuracy",
                color="Gender",
                color_discrete_map={"Female": "pink", "Male": "blue"},
                title="Model Accuracy by Gender"
            )
            st.plotly_chart(fig, use_container_width=True)

    # Disease-specific analysis
    if len(df_gender["Prediction"].unique()) > 2:  # More than just binary classification
        st.markdown("### Disease-Specific Gender Analysis")

        # Group by disease and gender
        disease_gender = df_gender.groupby(["Prediction", "Gender"]).size().unstack(fill_value=0)

        # Calculate percentages
        disease_gender_pct = disease_gender.div(disease_gender.sum(axis=1), axis=0) * 100

        # Display the tables
        col1, col2 = st.columns(2)

        with col1:
            st.write("**Disease counts by gender:**")
            st.dataframe(disease_gender)

        with col2:
            st.write("**Disease distribution by gender (%):**")
            st.dataframe(disease_gender_pct.round(1))

        # Visualize disease distribution by gender
        disease_data = []
        for disease, row in disease_gender.iterrows():
            for gender, count in row.items():
                disease_data.append({
                    "Disease": disease,
                    "Gender": gender,
                    "Count": count
                })

        disease_df = pd.DataFrame(disease_data)

        # Get top diseases for visualization
        top_diseases = disease_gender.sum(axis=1).nlargest(5).index.tolist()
        disease_df_top = disease_df[disease_df["Disease"].isin(top_diseases)]

        fig = px.bar(
            disease_df_top,
            x="Disease",
            y="Count",
            color="Gender",
            barmode="group",
            color_discrete_map={"F": "pink", "M": "blue"},
            title="Top 5 Diseases by Gender"
        )
        st.plotly_chart(fig, use_container_width=True)

    # Model comparison if multiple models were used
    if len(df_gender["Model"].unique()) > 1:
        st.markdown("### Bias Analysis by Model")

        models = df_gender["Model"].unique()
        model_bias = []

        for model in models:
            model_data = df_gender[df_gender["Model"] == model]

            # Skip if insufficient data
            if model_data.empty or "F" not in model_data["Gender"].values or "M" not in model_data["Gender"].values:
                continue

            # Calculate detection rates
            m_total_F = model_data[model_data["Gender"] == "F"].shape[0]
            m_total_M = model_data[model_data["Gender"] == "M"].shape[0]

            m_pos_F = model_data[(model_data["Gender"] == "F") & (model_data["Prediction"] != "No Disease")].shape[0]
            m_pos_M = model_data[(model_data["Gender"] == "M") & (model_data["Prediction"] != "No Disease")].shape[0]

            m_rate_F = m_pos_F / m_total_F if m_total_F > 0 else 0
            m_rate_M = m_pos_M / m_total_M if m_total_M > 0 else 0

            model_bias.append({
                "Model": model,
                "Female Rate": m_rate_F,
                "Male Rate": m_rate_M,
                "Bias Difference": abs(m_rate_F - m_rate_M)
            })

        if model_bias:
            model_bias_df = pd.DataFrame(model_bias)

            # Display model bias comparison
            st.write("**Model Bias Comparison:**")
            st.dataframe(model_bias_df.round(4))

            # Bar chart of bias by model
            fig = px.bar(
                model_bias_df,
                x="Model",
                y="Bias Difference",
                color="Model",
                title="Gender Bias by Model"
            )
            st.plotly_chart(fig, use_container_width=True)

    # Summary and recommendations
    st.markdown("### Bias Assessment Summary")

    if bias_diff > 0.15:
        st.error(
            "**Significant gender bias detected in the model predictions!**\n\n"
            "The model shows substantial disparity in how it treats different genders. "
            "Consider applying bias mitigation techniques in the next section."
        )
    elif bias_diff > 0.05:
        st.warning(
            "**Moderate gender bias detected in the model predictions.**\n\n"
            "There is some disparity in how the model treats different genders. "
            "Consider exploring bias mitigation techniques."
        )
    else:
        st.success(
            "**Low gender bias detected in the model predictions.**\n\n"
            "The model appears to be relatively fair across genders. "
            "Continue monitoring as new data becomes available."
        )

def bias_mitigation_simulation_page():
    st.title("🛠️ Bias Mitigation & Simulation")

    df = st.session_state.df
    df_results = st.session_state.df_results

    if df is None or df_results.empty:
        st.info("No prediction data available yet. Please make predictions first.")
        return

    # Remove rows with unknown gender
    df_results_clean = df_results[df_results["Gender"] != "Unknown"].copy()

    if df_results_clean.empty:
        st.warning("No predictions with known gender available for bias mitigation.")
        return

    # Introduction to bias mitigation
    st.markdown(
        """
        ### Introduction to Bias Mitigation

        Bias mitigation in AI aims to reduce unfair disparities between different demographic groups.
        Several techniques can be applied:

        1. **Threshold Adjustment**: Apply different decision thresholds for different groups
        2. **Reweighing**: Assign different weights to training instances from different groups
        3. **Adversarial Debiasing**: Use adversarial techniques to remove sensitive attributes
        4. **Fair Representations**: Learn representations that are independent of protected attributes
        5. **Post-Processing Calibration**: Adjust model outputs after prediction
        """
    )

    # Select mitigation approach
    mitigation_method = st.selectbox(
        "Select Mitigation Method:",
        ["Threshold Adjustment", "Reweighing", "Post-Processing Calibration"],
        help="Choose a method to mitigate gender bias"
    )

    st.markdown(f"### Apply {mitigation_method}")

    # Define specific parameters based on mitigation method
    if mitigation_method == "Threshold Adjustment":
        st.markdown(
            """
            **Threshold Adjustment** applies different classification thresholds to different groups
            to equalize error rates or positive prediction rates.
            """
        )

        # Get current thresholds
        current_threshold = 0.5  # Default threshold

        # Allow setting different thresholds for different genders
        col1, col2 = st.columns(2)

        with col1:
            female_threshold = st.slider(
                "Threshold for Female",
                0.0, 1.0, current_threshold, 0.01,
                help="Lower the threshold to increase positive predictions for females"
            )

        with col2:
            male_threshold = st.slider(
                "Threshold for Male",
                0.0, 1.0, current_threshold, 0.01,
                help="Higher the threshold to decrease positive predictions for males"
            )

        # Create binary predictions for visualization
        # NOTE: This is a simplified approach for demonstration
        df_results_clean["Binary_Prediction"] = (df_results_clean["Prediction"] != "No Disease").astype(int)

        # Apply thresholds based on gender
        df_results_clean["Mitigated_Prediction"] = df_results_clean.apply(
            lambda row: 1 if (row["Gender"] == "F" and row["Probability"] >= female_threshold) or
                              (row["Gender"] == "M" and row["Probability"] >= male_threshold)
                       else 0,
            axis=1
        )

    elif mitigation_method == "Reweighing":
        st.markdown(
            """
            **Reweighing** assigns different weights to training instances to ensure fairness.
            This method is typically applied during model training, but we can simulate its effect.
            """
        )

        # Calculate current positive rates
        female_df = df_results_clean[df_results_clean["Gender"] == "F"]
        male_df = df_results_clean[df_results_clean["Gender"] == "M"]

        # Convert prediction to binary for easier calculation
        df_results_clean["Binary_Prediction"] = (df_results_clean["Prediction"] != "No Disease").astype(int)

        # Calculate positive rates
        female_pos_rate = female_df["Binary_Prediction"].mean()
        male_pos_rate = male_df["Binary_Prediction"].mean()

        # Display current rates
        st.write(f"**Current female positive rate:** {female_pos_rate:.2%}")
        st.write(f"**Current male positive rate:** {male_pos_rate:.2%}")

        # Weight factor to adjust positive rates
        weight_factor = st.slider(
            "Adjustment Factor",
            0.0, 2.0, 1.0, 0.1,
            help="Values < 1 reduce disparity by decreasing the higher rate, values > 1 increase the lower rate"
        )

        # Apply weights (simplified simulation)
        if female_pos_rate > male_pos_rate:
            # Reduce female positive rate
            female_adj = 1 / weight_factor if weight_factor > 0 else 1
            male_adj = 1
        else:
            # Reduce male positive rate
            female_adj = 1
            male_adj = 1 / weight_factor if weight_factor > 0 else 1

        # Apply weighted predictions (simplified simulation)
        df_results_clean["Mitigated_Prediction"] = df_results_clean.apply(
            lambda row: 1 if (row["Gender"] == "F" and row["Binary_Prediction"] == 1 and np.random.random() <= female_adj) or
                              (row["Gender"] == "M" and row["Binary_Prediction"] == 1 and np.random.random() <= male_adj)
                       else 0,
            axis=1
        )

    elif mitigation_method == "Post-Processing Calibration":
        st.markdown(
            """
            **Post-Processing Calibration** adjusts predictions after the model has been trained
            to ensure equal error rates or equal positive rates across groups.
            """
        )

        # Convert prediction to binary for easier calculation
        df_results_clean["Binary_Prediction"] = (df_results_clean["Prediction"] != "No Disease").astype(int)

        # Current positive rates
        female_df = df_results_clean[df_results_clean["Gender"] == "F"]
        male_df = df_results_clean[df_results_clean["Gender"] == "M"]

        female_pos_rate = female_df["Binary_Prediction"].mean()
        male_pos_rate = male_df["Binary_Prediction"].mean()

        # Display current rates
        st.write(f"**Current female positive rate:** {female_pos_rate:.2%}")
        st.write(f"**Current male positive rate:** {male_pos_rate:.2%}")

        # Target equal positive rates
        target_rate = st.slider(
            "Target Positive Rate",
            min(female_pos_rate, male_pos_rate),
            max(female_pos_rate, male_pos_rate),
            (female_pos_rate + male_pos_rate) / 2,
            0.01,
            help="Target equal positive prediction rate across genders"
        )

        # Apply calibration (simplified)
        # This randomly flips predictions to achieve the target rate
        df_results_clean["Mitigated_Prediction"] = df_results_clean["Binary_Prediction"].copy()

        # For females
        if female_pos_rate != target_rate:
            female_indices = df_results_clean[df_results_clean["Gender"] == "F"].index
            female_preds = df_results_clean.loc[female_indices, "Binary_Prediction"].values

            num_female = len(female_indices)
            current_pos = int(female_pos_rate * num_female)
            target_pos = int(target_rate * num_female)

            if current_pos < target_pos:
                # Need to flip some 0s to 1s
                flip_indices = np.random.choice(
                    female_indices[female_preds == 0],
                    target_pos - current_pos,
                    replace=False
                )
                df_results_clean.loc[flip_indices, "Mitigated_Prediction"] = 1
            else:
                # Need to flip some 1s to 0s
                flip_indices = np.random.choice(
                    female_indices[female_preds == 1],
                    current_pos - target_pos,
                    replace=False
                )
                df_results_clean.loc[flip_indices, "Mitigated_Prediction"] = 0

        # For males
        if male_pos_rate != target_rate:
            male_indices = df_results_clean[df_results_clean["Gender"] == "M"].index
            male_preds = df_results_clean.loc[male_indices, "Binary_Prediction"].values

            num_male = len(male_indices)
            current_pos = int(male_pos_rate * num_male)
            target_pos = int(target_rate * num_male)

            if current_pos < target_pos:
                # Need to flip some 0s to 1s
                flip_indices = np.random.choice(
                    male_indices[male_preds == 0],
                    target_pos - current_pos,
                    replace=False
                )
                df_results_clean.loc[flip_indices, "Mitigated_Prediction"] = 1
            else:
                # Need to flip some 1s to 0s
                flip_indices = np.random.choice(
                    male_indices[male_preds == 1],
                    current_pos - target_pos,
                    replace=False
                )
                df_results_clean.loc[flip_indices, "Mitigated_Prediction"] = 0

    # Compute and display results of mitigation
    st.markdown("### Mitigation Results")

    # Ensure binary predictions for evaluation
    if "Binary_Prediction" not in df_results_clean.columns:
        df_results_clean["Binary_Prediction"] = (df_results_clean["Prediction"] != "No Disease").astype(int)

    # Compute metrics before mitigation
    female_before = df_results_clean[df_results_clean["Gender"] == "F"]["Binary_Prediction"].mean()
    male_before = df_results_clean[df_results_clean["Gender"] == "M"]["Binary_Prediction"].mean()
    disparity_before = abs(female_before - male_before)

    # Compute metrics after mitigation
    female_after = df_results_clean[df_results_clean["Gender"] == "F"]["Mitigated_Prediction"].mean()
    male_after = df_results_clean[df_results_clean["Gender"] == "M"]["Mitigated_Prediction"].mean()
    disparity_after = abs(female_after - male_after)

    # Display comparison
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**Before Mitigation**")
        st.write(f"Female positive rate: {female_before:.2%}")
        st.write(f"Male positive rate: {male_before:.2%}")
        st.write(f"Gender disparity: {disparity_before:.4f}")

    with col2:
        st.markdown("**After Mitigation**")
        st.write(f"Female positive rate: {female_after:.2%}")
        st.write(f"Male positive rate: {male_after:.2%}")
        st.write(f"Gender disparity: {disparity_after:.4f}")

    # Improvement
    improvement = (1 - disparity_after/disparity_before) * 100 if disparity_before > 0 else 0
    st.markdown(f"**Disparity reduction:** {improvement:.1f}%")

    # Visualize the changes
    comparison_data = pd.DataFrame({
        'Stage': ['Before', 'Before', 'After', 'After'],
        'Gender': ['Female', 'Male', 'Female', 'Male'],
        'Positive Rate': [female_before, male_before, female_after, male_after]
    })

    fig = px.bar(
        comparison_data,
        x="Stage",
        y="Positive Rate",
        color="Gender",
        barmode="group",
        color_discrete_map={"Female": "pink", "Male": "blue"},
        title="Impact of Bias Mitigation"
    )
    st.plotly_chart(fig, use_container_width=True)

    # Performance impact assessment
    if "Actual" in df_results_clean.columns and not all(df_results_clean["Actual"] == "Unknown"):
        st.markdown("### Performance Impact Assessment")

        # Binarize actual values
        df_results_clean["Binary_Actual"] = (df_results_clean["Actual"] != "No Disease").astype(int)

        # Calculate accuracy before
        accuracy_before = accuracy_score(df_results_clean["Binary_Actual"], df_results_clean["Binary_Prediction"])

        # Calculate accuracy after
        accuracy_after = accuracy_score(df_results_clean["Binary_Actual"], df_results_clean["Mitigated_Prediction"])

        # Display accuracy comparison
        col1, col2 = st.columns(2)

        with col1:
            st.metric("Accuracy before mitigation", f"{accuracy_before:.2%}")

        with col2:
            st.metric("Accuracy after mitigation", f"{accuracy_after:.2%}",
                     delta=f"{(accuracy_after - accuracy_before) * 100:.2f}%")

        # Warning about accuracy-fairness tradeoff if applicable
        if accuracy_after < accuracy_before:
            st.warning(
                "⚠️ **Note the accuracy-fairness tradeoff**\n\n"
                "Bias mitigation has reduced overall accuracy. This is a common trade-off "
                "in fair machine learning. Consider the specific context and ethical implications "
                "when deciding whether this trade-off is acceptable."
            )
        else:
            st.success(
                "✅ **Bias mitigation improved both fairness and accuracy**\n\n"
                "This is an ideal outcome where reducing bias also leads to better performance."
            )

        # Confusion matrices
        st.markdown("#### Confusion Matrices")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**Before Mitigation**")
            cm_before = confusion_matrix(df_results_clean["Binary_Actual"], df_results_clean["Binary_Prediction"])

            # Plot confusion matrix
            fig, ax = plt.subplots(figsize=(4, 3))
            im = ax.imshow(cm_before, interpolation='nearest', cmap=plt.cm.Blues)
            ax.set(xticks=[0, 1], yticks=[0, 1],
                   xticklabels=["No Disease", "Disease"],
                   yticklabels=["No Disease", "Disease"],
                   xlabel="Predicted", ylabel="Actual")

            # Add text annotations
            for i in range(2):
                for j in range(2):
                    ax.text(j, i, cm_before[i, j], ha="center", va="center")

            st.pyplot(fig)

        with col2:
            st.markdown("**After Mitigation**")
            cm_after = confusion_matrix(df_results_clean["Binary_Actual"], df_results_clean["Mitigated_Prediction"])

            # Plot confusion matrix
            fig, ax = plt.subplots(figsize=(4, 3))
            im = ax.imshow(cm_after, interpolation='nearest', cmap=plt.cm.Blues)
            ax.set(xticks=[0, 1], yticks=[0, 1],
                   xticklabels=["No Disease", "Disease"],
                   yticklabels=["No Disease", "Disease"],
                   xlabel="Predicted", ylabel="Actual")

            # Add text annotations
            for i in range(2):
                for j in range(2):
                    ax.text(j, i, cm_after[i, j], ha="center", va="center")

            st.pyplot(fig)

    # Recommendations
    st.markdown("### Recommendations")

    if disparity_after < 0.05:
        st.success(
            "✅ **Bias successfully mitigated**\n\n"
            "The mitigation approach has effectively reduced gender disparity in the model predictions. "
            "Consider implementing this approach in your production system."
        )
    elif disparity_after < 0.1:
        st.success(
            "✅ **Bias substantially reduced**\n\n"
            "The mitigation approach has significantly reduced gender disparity, though some small differences remain. "
            "This level of disparity may be acceptable depending on your specific use case and regulatory requirements."
        )
    else:
        st.warning(
            "⚠️ **Partial bias mitigation achieved**\n\n"
            "While the disparity has been reduced, significant differences between genders still exist. "
            "Consider trying different mitigation approaches or parameters."
        )

    # Display the mitigated predictions
    st.markdown("### Mitigated Predictions")
    st.dataframe(df_results_clean[["Image_ID", ""Gender", "Prediction", "Probability", "Mitigated_Prediction"]])

    # Save button for mitigated predictions
    if st.button("Save Mitigated Predictions"):
        df_results_clean_copy = df_results_clean.copy()

        # Map binary predictions back to disease labels
        def map_to_disease(row):
            if row["Mitigated_Prediction"] == 1:
                return row["Prediction"] if row["Prediction"] != "No Disease" else "Disease"
            else:
                return "No Disease"

        df_results_clean_copy["Mitigated_Disease"] = df_results_clean_copy.apply(map_to_disease, axis=1)

        # Replace original predictions in the results
        for idx, row in df_results_clean_copy.iterrows():
            mask = (st.session_state.df_results["Image_ID"] == row["Image_ID"]) & (st.session_state.df_results["Gender"] == row["Gender"])
            st.session_state.df_results.loc[mask, "Prediction"] = row["Mitigated_Disease"]

        st.success("✅ Mitigated predictions saved successfully!")

def gender_bias_testing_page():
    st.title("🧪 Gender Bias Testing")

    df_results = st.session_state.df_results
    if df_results.empty:
        st.info("No prediction data available. Generate predictions first.")
        return

    st.markdown(
        """
        ### Interactive Bias Testing Environment

        This environment allows you to experiment with different parameters and approaches
        to understand and mitigate gender bias in model predictions.
        """
    )

    # Remove rows with unknown gender
    df_results_clean = df_results[df_results["Gender"] != "Unknown"].copy()

    if df_results_clean.empty:
        st.warning("No predictions with known gender available for testing.")
        return

    # Testing method selection
    test_method = st.radio(
        "Select Testing Method:",
        ["Threshold Adjustment", "Data Rebalancing Simulation", "Model Ensemble Simulation"],
        help="Choose a method to test bias mitigation approaches"
    )

    if test_method == "Threshold Adjustment":
        st.markdown(
            """
            ### Threshold Adjustment Testing

            Threshold adjustment is a post-processing technique that applies different classification
            thresholds to different demographic groups to balance error rates or prediction rates.
            """
        )

        col1, col2 = st.columns(2)

        with col1:
            thresh_F = st.slider(
                "Threshold for Female",
                0.0, 1.0, 0.5, 0.01,
                help="Adjust threshold for female predictions"
            )

        with col2:
            thresh_M = st.slider(
                "Threshold for Male",
                0.0, 1.0, 0.5, 0.01,
                help="Adjust threshold for male predictions"
            )

        # Binary predictions for testing
        df_results_clean["Binary_Prediction"] = (df_results_clean["Prediction"] != "No Disease").astype(int)

        # Apply adjusted thresholds
        def adjust_pred(row):
            if row["Gender"] == "M":
                return 1 if row["Probability"] >= thresh_M else 0
            elif row["Gender"] == "F":
                return 1 if row["Probability"] >= thresh_F else 0
            else:
                return row["Binary_Prediction"]

        df_results_clean["Adjusted_Prediction"] = df_results_clean.apply(adjust_pred, axis=1)

        # Calculate metrics
        female_df = df_results_clean[df_results_clean["Gender"] == "F"]
        male_df = df_results_clean[df_results_clean["Gender"] == "M"]

        total_F = len(female_df)
        total_M = len(male_df)

        # Original rates
        orig_F_rate = female_df["Binary_Prediction"].mean()
        orig_M_rate = male_df["Binary_Prediction"].mean()
        orig_diff = abs(orig_F_rate - orig_M_rate)

        # Adjusted rates
        adj_F_rate = female_df["Adjusted_Prediction"].mean()
        adj_M_rate = male_df["Adjusted_Prediction"].mean()
        adj_diff = abs(adj_F_rate - adj_M_rate)

        # Display results
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**Original Prediction Rates**")
            st.write(f"Female detection rate: {orig_F_rate:.2%} ({total_F} images)")
            st.write(f"Male detection rate: {orig_M_rate:.2%} ({total_M} images)")
            st.write(f"Bias difference: {orig_diff:.4f}")

        with col2:
            st.markdown("**Adjusted Prediction Rates**")
            st.write(f"Female detection rate: {adj_F_rate:.2%} ({total_F} images)")
            st.write(f"Male detection rate: {adj_M_rate:.2%} ({total_M} images)")
            st.write(f"Bias difference: {adj_diff:.4f}")

        # Visualization of changes
        data = pd.DataFrame({
            'Stage': ['Original', 'Original', 'Adjusted', 'Adjusted'],
            'Gender': ['Female', 'Male', 'Female', 'Male'],
            'Detection Rate': [orig_F_rate, orig_M_rate, adj_F_rate, adj_M_rate]
        })

        fig = px.bar(
            data,
            x="Stage",
            y="Detection Rate",
            color="Gender",
            barmode="group",
            color_discrete_map={"Female": "pink", "Male": "blue"},
            title="Impact of Threshold Adjustment"
        )
        st.plotly_chart(fig, use_container_width=True)

        # Assess bias reduction
        if adj_diff < orig_diff:
            reduction = ((orig_diff - adj_diff) / orig_diff) * 100
            st.success(f"✅ Bias reduced by {reduction:.1f}%")
        else:
            st.error("❌ Bias not reduced. Try different threshold values.")

    elif test_method == "Data Rebalancing Simulation":
        st.markdown(
            """
            ### Data Rebalancing Simulation

            This simulation tests how rebalancing the training data might affect model bias.
            In practice, this would involve retraining with a balanced dataset, but we'll
            simulate the effect here.
            """
        )

        # Current gender distribution
        gender_counts = df_results_clean["Gender"].value_counts()
        total_F = gender_counts.get("F", 0)
        total_M = gender_counts.get("M", 0)

        st.write(f"**Current dataset:** {total_F} females, {total_M} males")
        ratio = total_F / total_M if total_M > 0 else float('inf')
        st.write(f"**Current female-to-male ratio:** {ratio:.2f}")

        # Simulation parameters
        target_ratio = st.slider(
            "Target Female-to-Male Ratio",
            min_value=0.5,
            max_value=2.0,
            value=1.0,
            step=0.1,
            help="1.0 represents a balanced dataset"
        )

        # Simulate rebalancing by weighted sampling
        df_results_clean["Binary_Prediction"] = (df_results_clean["Prediction"] != "No Disease").astype(int)

        # Apply simulation and show results
        st.dataframe(df_results_clean[["Image_ID", "Gender", "Prediction", "Probability", "Binary_Prediction"]])

    elif test_method == "Model Ensemble Simulation":
        st.markdown(
            """
            ### Model Ensemble Simulation

            This simulation tests how combining predictions from multiple models might
            reduce bias. In practice, this would involve training multiple specialized
            models, but we'll simulate the effect here.
            """
        )

        # Simulation controls and displays
        st.write("Available models in ensemble:")

        for model_name in st.session_state.models_loaded.keys():
            st.checkbox(model_name, value=True, key=f"use_{model_name}")

def explainable_analysis_page():
    st.title("🔍 Explainable Analysis")

    df = st.session_state.df
    df_results = st.session_state.df_results

    if df is None or df_results.empty:
        st.info("No prediction data available yet. Please make predictions first.")
        return

    st.markdown(
        """
        ### Explainable AI for Chest X-Ray Interpretation

        Understanding how models make decisions is crucial for:

        1. **Clinical Trust**: Helping radiologists verify model decisions
        2. **Bias Detection**: Identifying when models use irrelevant features
        3. **Error Analysis**: Understanding failure modes
        4. **Educational Value**: Learning from the model's attention patterns
        """
    )

    # Simulated explanation methods
    explanation_method = st.selectbox(
        "Select Explanation Method:",
        ["Confusion Matrix Analysis", "Error Pattern Analysis", "Demographic Analysis"]
    )

    if explanation_method == "Confusion Matrix Analysis":
        st.markdown("### Confusion Matrix Analysis")

        if "Actual" not in df_results.columns or all(df_results["Actual"] == "Unknown"):
            st.warning("Ground truth labels required for confusion matrix analysis.")
            st.info("Please ensure your dataset has actual disease labels matched to your predictions.")
            return

        # Get binary predictions
        df_analysis = df_results.copy()
        df_analysis["Binary_Actual"] = (df_analysis["Actual"] != "No Disease").astype(int)
        df_analysis["Binary_Prediction"] = (df_analysis["Prediction"] != "No Disease").astype(int)

        # Create overall confusion matrix
        cm = confusion_matrix(df_analysis["Binary_Actual"], df_analysis["Binary_Prediction"])

        # Display confusion matrix
        st.markdown("#### Overall Confusion Matrix")

        fig, ax = plt.subplots(figsize=(6, 5))
        cax = ax.matshow(cm, cmap=plt.cm.Blues)
        fig.colorbar(cax)

        ax.set_xlabel("Predicted")
        ax.set_ylabel("Actual")
        ax.set_title("Confusion Matrix")

        ax.set_xticks([0, 1])
        ax.set_yticks([0, 1])
        ax.set_xticklabels(["No Disease", "Disease"])
        ax.set_yticklabels(["No Disease", "Disease"])

        # Add text annotations
        for (i, j), val in np.ndenumerate(cm):
            ax.text(j, i, f"{val}", ha="center", va="center", color="white" if val > cm.max()/2 else "black")

        st.pyplot(fig)

        # Calculate metrics
        tn, fp, fn, tp = cm.ravel()
        accuracy = (tp + tn) / (tp + tn + fp + fn)
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0
        npv = tn / (tn + fn) if (tn + fn) > 0 else 0

        # Display metrics
        st.markdown("#### Performance Metrics")

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Accuracy", f"{accuracy:.2%}")
            st.metric("Sensitivity", f"{sensitivity:.2%}")
        with col2:
            st.metric("Specificity", f"{specificity:.2%}")
            st.metric("PPV", f"{ppv:.2%}")
        with col3:
            st.metric("NPV", f"{npv:.2%}")
            st.metric("F1 Score", f"{2*ppv*sensitivity/(ppv+sensitivity) if (ppv+sensitivity) > 0 else 0:.2%}")

        # Gender-specific analysis
        st.markdown("#### Gender-Specific Confusion Matrices")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**Female Patients**")
            female_df = df_analysis[df_analysis["Gender"] == "F"]
            if not female_df.empty:
                cm_f = confusion_matrix(female_df["Binary_Actual"], female_df["Binary_Prediction"])

                fig_f, ax_f = plt.subplots(figsize=(4, 4))
                cax_f = ax_f.matshow(cm_f, cmap=plt.cm.PuRd)
                fig_f.colorbar(cax_f)

                ax_f.set_xticks([0, 1])
                ax_f.set_yticks([0, 1])
                ax_f.set_xticklabels(["No Disease", "Disease"])
                ax_f.set_yticklabels(["No Disease", "Disease"])

                # Add text annotations
                for (i, j), val in np.ndenumerate(cm_f):
                    ax_f.text(j, i, f"{val}", ha="center", va="center",
                             color="white" if val > cm_f.max()/2 else "black")

                st.pyplot(fig_f)
            else:
                st.info("No female patients with both predictions and actual labels.")

        with col2:
            st.markdown("**Male Patients**")
            male_df = df_analysis[df_analysis["Gender"] == "M"]
            if not male_df.empty:
                cm_m = confusion_matrix(male_df["Binary_Actual"], male_df["Binary_Prediction"])

                fig_m, ax_m = plt.subplots(figsize=(4, 4))
                cax_m = ax_m.matshow(cm_m, cmap=plt.cm.Blues)
                fig_m.colorbar(cax_m)

                ax_m.set_xticks([0, 1])
                ax_m.set_yticks([0, 1])
                ax_m.set_xticklabels(["No Disease", "Disease"])
                ax_m.set_yticklabels(["No Disease", "Disease"])

                # Add text annotations
                for (i, j), val in np.ndenumerate(cm_m):
                    ax_m.text(j, i, f"{val}", ha="center", va="center",
                             color="white" if val > cm_m.max()/2 else "black")

                st.pyplot(fig_m)
            else:
                st.info("No male patients with both predictions and actual labels.")

    elif explanation_method == "Error Pattern Analysis":
        st.markdown("### Error Pattern Analysis")

        if "Actual" not in df_results.columns or all(df_results["Actual"] == "Unknown"):
            st.warning("Ground truth labels required for error pattern analysis.")
            return

        # Get binary predictions and errors
        df_analysis = df_results.copy()
        df_analysis["Binary_Actual"] = (df_analysis["Actual"] != "No Disease").astype(int)
        df_analysis["Binary_Prediction"] = (df_analysis["Prediction"] != "No Disease").astype(int)
        df_analysis["Correct"] = df_analysis["Binary_Actual"] == df_analysis["Binary_Prediction"]

        # Error types
        df_analysis["Error_Type"] = "Correct"
        df_analysis.loc[(df_analysis["Binary_Actual"] == 1) & (df_analysis["Binary_Prediction"] == 0), "Error_Type"] = "False Negative"
        df_analysis.loc[(df_analysis["Binary_Actual"] == 0) & (df_analysis["Binary_Prediction"] == 1), "Error_Type"] = "False Positive"

        # Overall error rate
        error_rate = (df_analysis["Error_Type"] != "Correct").mean()
        st.metric("Overall Error Rate", f"{error_rate:.2%}")

        # Error distribution
        error_counts = df_analysis["Error_Type"].value_counts()

        # Create pie chart
        fig = px.pie(
            values=error_counts.values,
            names=error_counts.index,
            title="Distribution of Predictions and Errors",
            color=error_counts.index,
            color_discrete_map={
                "Correct": "#36b9cc",
                "False Positive": "#e74a3b",
                "False Negative": "#f6c23e"
            }
        )
        st.plotly_chart(fig, use_container_width=True)

        # Gender-based error analysis
        st.markdown("#### Error Analysis by Gender")

        # Group by gender and error type
        gender_errors = pd.crosstab(
            df_analysis["Gender"],
            df_analysis["Error_Type"],
            normalize="index"
        ) * 100

        # Add sample sizes
        gender_sizes = df_analysis["Gender"].value_counts()
        gender_error_counts = pd.crosstab(df_analysis["Gender"], df_analysis["Error_Type"])

        # Display tables
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**Error Percentages by Gender**")
            st.dataframe(gender_errors.round(1))

        with col2:
            st.markdown("**Error Counts by Gender**")
            st.dataframe(gender_error_counts)

        # Visualize error rates by gender
        gender_error_df = gender_errors.reset_index().melt(
            id_vars="Gender",
            var_name="Error_Type",
            value_name="Percentage"
        )

        # Create bar chart
        fig = px.bar(
            gender_error_df,
            x="Gender",
            y="Percentage",
            color="Error_Type",
            barmode="group",
            title="Error Types by Gender",
            color_discrete_map={
                "Correct": "#36b9cc",
                "False Positive": "#e74a3b",
                "False Negative": "#f6c23e"
            }
        )
        st.plotly_chart(fig, use_container_width=True)

    elif explanation_method == "Demographic Analysis":
        st.markdown("### Demographic Analysis")

        df_analysis = df_results.copy()

        # Check if age data is available
        if st.session_state.age_col and st.session_state.age_col in df.columns:
            st.markdown("#### Age Distribution Analysis")

            # Merge prediction results with demographic data
            id_col = st.session_state.image_id_col
            age_col = st.session_state.age_col

            # Try to match images with demographic data
            merged_df = pd.merge(
                df_analysis,
                df[[id_col, age_col, st.session_state.gender_col]],
                left_on="Image_ID",
                right_on=id_col,
                how="left"
            )

            # Convert age to numeric, handling errors
            merged_df[age_col] = pd.to_numeric(merged_df[age_col], errors="coerce")

            # Remove rows with missing age
            merged_df = merged_df.dropna(subset=[age_col])

            if not merged_df.empty:
                # Create age groups
                merged_df["Age_Group"] = pd.cut(
                    merged_df[age_col],
                    bins=[0, 18, 40, 60, 80, 120],
                    labels=["0-18", "19-40", "41-60", "61-80", "81+"]
                )

                # Binary predictions for analysis
                merged_df["Binary_Prediction"] = (merged_df["Prediction"] != "No Disease").astype(int)

                # Positive prediction rate by age group
                age_group_rates = merged_df.groupby("Age_Group")["Binary_Prediction"].mean().reset_index()
                age_group_rates["Positive_Rate"] = age_group_rates["Binary_Prediction"] * 100

                # Plot positive prediction rates by age group
                fig = px.bar(
                    age_group_rates,
                    x="Age_Group",
                    y="Positive_Rate",
                    title="Positive Prediction Rate by Age Group",
                    labels={"Positive_Rate": "Positive Rate (%)", "Age_Group": "Age Group"},
                    color="Positive_Rate",
                    color_continuous_scale="Blues"
                )
                st.plotly_chart(fig, use_container_width=True)

                # Age distribution by gender
                st.markdown("#### Age Distribution by Gender")

                fig = px.box(
                    merged_df,
                    x="Gender",
                    y=age_col,
                    color="Gender",
                    title="Age Distribution by Gender",
                    color_discrete_map={"F": "pink", "M": "blue", "Unknown": "gray"}
                )
                st.plotly_chart(fig, use_container_width=True)

                # Age and gender interaction analysis
                st.markdown("#### Age and Gender Interaction Analysis")

                # Positive prediction rate by age group and gender
                age_gender_rates = merged_df.groupby(["Age_Group", "Gender"])["Binary_Prediction"].mean().reset_index()
                age_gender_rates["Positive_Rate"] = age_gender_rates["Binary_Prediction"] * 100

                # Plot positive prediction rates by age group and gender
                fig = px.bar(
                    age_gender_rates,
                    x="Age_Group",
                    y="Positive_Rate",
                    color="Gender",
                    barmode="group",
                    title="Positive Prediction Rate by Age Group and Gender",
                    labels={"Positive_Rate": "Positive Rate (%)", "Age_Group": "Age Group"},
                    color_discrete_map={"F": "pink", "M": "blue", "Unknown": "gray"}
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("No age data available for demographic analysis.")
        else:
            st.info("Age column not selected or not available in the dataset. Please include age data for demographic analysis.")

        # Gender analysis
        st.markdown("#### Gender Representation Analysis")

        # Gender distribution
        gender_counts = df_analysis["Gender"].value_counts()
        gender_percentages = (gender_counts / gender_counts.sum() * 100).round(1)

        # Create pie chart
        fig = px.pie(
            values=gender_counts.values,
            names=gender_counts.index,
            title="Gender Distribution in Analyzed Data",
            color=gender_counts.index,
            color_discrete_map={"F": "pink", "M": "blue", "Unknown": "gray"}
        )
        fig.update_traces(texttemplate="%{percent:.1f}%")
        st.plotly_chart(fig, use_container_width=True)

        # Check for gender imbalance
        if "F" in gender_percentages.index and "M" in gender_percentages.index:
            imbalance = abs(gender_percentages["F"] - gender_percentages["M"])

            if imbalance > 20:
                st.error(
                    f"⚠️ **Significant gender imbalance detected: {imbalance:.1f}% difference**\n\n"
                    "This dataset has a substantial gender imbalance which may contribute to biased model performance. "
                    "Consider rebalancing your dataset for more equitable representation."
                )
            elif imbalance > 10:
                st.warning(
                    f"⚠️ **Moderate gender imbalance detected: {imbalance:.1f}% difference**\n\n"
                    "This dataset has some gender imbalance which may affect model performance across genders. "
                    "Consider whether this imbalance reflects the target population."
                )
            else:
                st.success(
                    f"✅ **Minimal gender imbalance detected: {imbalance:.1f}% difference**\n\n"
                    "The dataset has relatively balanced gender representation, which helps reduce bias."
                )

def about_densenet_model_page():
    st.title("🧠 About DenseNet121 Model")

    st.markdown(
        """
        ## DenseNet121: Deep Convolutional Network for Chest X-ray Classification

        **DenseNet121** is a convolutional neural network architecture that incorporates dense connectivity
        patterns to improve feature propagation, encourage feature reuse, and substantially reduce the
        number of parameters.

        ### Model Architecture

        The DenseNet121 architecture includes:

        - **121 layers** with convolutional and pooling operations
        - **Dense connectivity** where each layer receives feature maps from all preceding layers
        - **Transition layers** (compression) between dense blocks
        - **Global average pooling** followed by a fully connected layer

        ### Implementation Details

        This implementation uses:

        - PyTorch's pre-trained DenseNet121 (trained on ImageNet)
        - Transfer learning by replacing the final classification layer
        - Input resolution of 224×224 pixels
        - Standard ImageNet normalization

        ### Training Process

        The base model is pre-trained on ImageNet and then fine-tuned for chest X-ray classification:

        1. The original ImageNet classifier (1000 classes) is replaced with a new classifier for chest pathologies
        2. The model is trained using a weighted binary cross-entropy loss
        3. Augmentation techniques are applied to reduce overfitting

        ### Performance

        DenseNet121-based chest X-ray models typically achieve:

        - AUC > 0.80 for common thoracic diseases
        - Comparable performance to radiologists for certain conditions
        - Good generalization across different datasets

        ### Advantages

        - Efficient parameter usage through feature reuse
        - Strong gradient flow through dense connections
        - Reduced vanishing gradient problems
        - Lower computational requirements than many comparable architectures

        ### Research Citation

        > G. Huang, Z. Liu, L. Van Der Maaten and K. Q. Weinberger, "Densely Connected Convolutional Networks," 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, 2017, pp. 4700-4708.
        """
    )

    # Add visualization of model architecture
    st.markdown("### DenseNet121 Architecture")

    col1, col2 = st.columns([1, 2])

    with col1:
        st.markdown(
            """
            **DenseNet-121 Structure:**

            1. Input layer (224×224 image)
            2. Initial convolution and pooling
            3. Dense blocks with transition layers:
               - Dense Block 1 (6 layers)
               - Transition Layer 1
               - Dense Block 2 (12 layers)
               - Transition Layer 2
               - Dense Block 3 (24 layers)
               - Transition Layer 3
               - Dense Block 4 (16 layers)
            4. Global Average Pooling
            5. Fully Connected Layer
            6. Sigmoid activation (multi-label output)
            """
        )

    with col2:
        # Display an ASCII art diagram of the architecture
        st.code("""
        Input Image (224x224)
            │
            ▼
        Convolution + Pooling
            │
            ▼
        ┌─────────────────┐
        │  Dense Block 1  │
        │   (6 layers)    │
        └─────────────────┘
            │
            ▼
        Transition Layer 1
            │
            ▼
        ┌─────────────────┐
        │  Dense Block 2  │
        │   (12 layers)   │
        └─────────────────┘
            │
            ▼
        Transition Layer 2
            │
            ▼
        ┌─────────────────┐
        │  Dense Block 3  │
        │   (24 layers)   │
        └─────────────────┘
            │
            ▼
        Transition Layer 3
            │
            ▼
        ┌─────────────────┐
        │  Dense Block 4  │
        │   (16 layers)   │
        └─────────────────┘
            │
            ▼
        Global Average Pooling
            │
            ▼
        Fully Connected Layer
            │
            ▼
        Sigmoid Activation
            │
            ▼
        Disease Predictions
        """)

    # Add a load button
    if st.button("Load DenseNet121 Model"):
        with st.spinner("Loading DenseNet121..."):
            if load_model("DenseNet121"):
                st.success("✅ DenseNet121 loaded successfully!")
            else:
                st.error("❌ Failed to load DenseNet121. Please try again.")

def about_resnet_model_page():
    st.title("🧠 About ResNet50 Model")

    st.markdown(
        """
        ## ResNet50: Deep Residual Network for Medical Image Analysis

        **ResNet50** is a deep convolutional neural network architecture that introduced residual learning
        to solve the vanishing gradient problem in very deep networks. It allows training of much deeper
        networks than was previously possible.

        ### Key Innovation: Residual Learning

        The core innovation in ResNet is the **residual block**:

        - Each block learns the residual (difference) between input and output
        - Identity shortcut connections allow gradients to flow directly through the network
        - This enables training of very deep networks (50+ layers) without degradation

        ### Model Architecture

        ResNet50 consists of:

        - Initial 7×7 convolution and pooling
        - 50 layers organized into 4 stages with residual blocks
        - Each stage doubles the number of filters and halves spatial dimensions
        - Global average pooling and fully connected layer for classification

        ### Implementation for Chest X-rays

        This implementation uses:

        - PyTorch's pre-trained ResNet50 (trained on ImageNet)
        - Transfer learning by replacing the final fully connected layer
        - Input resolution of 224×224 pixels
        - Standard ImageNet normalization

        ### Adaptation for Medical Imaging

        For chest X-ray analysis:

        - The final 1000-class ImageNet classifier is replaced with a new classifier for chest pathologies
        - Early layers (which detect basic features) are preserved from ImageNet pre-training
        - Later layers are fine-tuned to capture domain-specific features

        ### Performance Characteristics

        ResNet50 chest X-ray classifiers typically show:

        - Strong performance on large pathologies (cardiomegaly, pleural effusion)
        - Good ability to generalize across different X-ray datasets
        - Computational efficiency compared to other models with similar performance

        ### Research Citation

        > K. He, X. Zhang, S. Ren and J. Sun, "Deep Residual Learning for Image Recognition," 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, 2016, pp. 770-778.
        """
    )

    # Add visualization of the ResNet architecture
    st.markdown("### ResNet50 Architecture")

    col1, col2 = st.columns([1, 2])

    with col1:
        st.markdown(
            """
            **ResNet50 Structure:**

            1. Input layer (224×224 image)
            2. 7×7 Conv, 64 filters
            3. Max Pooling
            4. Stage 1: 3 residual blocks
            5. Stage 2: 4 residual blocks
            6. Stage 3: 6 residual blocks
            7. Stage 4: 3 residual blocks
            8. Global Average Pooling
            9. Fully Connected Layer
            10. Sigmoid activation
            """
        )

    with col2:
        # Display an ASCII art diagram of the architecture
        st.code("""
        Input Image (224x224)
            │
            ▼
        7×7 Conv, 64 + Max Pool
            │
            ▼
        ┌─────────────────┐
        │    Stage 1      │
        │  3 Res Blocks   │
        └─────────────────┘
            │
            ▼
        ┌─────────────────┐
        │    Stage 2      │
        │  4 Res Blocks   │
        └─────────────────┘
            │
            ▼
        ┌─────────────────┐
        │    Stage 3      │
        │  6 Res Blocks   │
        └─────────────────┘
            │
            ▼
        ┌─────────────────┐
        │    Stage 4      │
        │  3 Res Blocks   │
        └─────────────────┘
            │
            ▼
        Global Average Pooling
            │
            ▼
        Fully Connected Layer
            │
            ▼
        Sigmoid Activation
            │
            ▼
        Disease Predictions
        """)

    # Visualization of a residual block
    st.markdown("#### Residual Block Structure")
    st.code("""
         Input
           │
    ┌──────┴──────┐
    │             │
    ▼             │
    Conv          │
    │             │
    ▼             │
    BatchNorm     │
    │             │
    ▼             │
    ReLU          │
    │             │
    ▼             │
    Conv          │
    │             │
    ▼             │
    BatchNorm     │
    │             │
    └──────┬──────┘
           │
           ▼
         Add
           │
           ▼
         ReLU
           │
           ▼
        Output
    """)

    # Add a load button
    if st.button("Load ResNet50 Model"):
        with st.spinner("Loading ResNet50..."):
            if load_model("ResNet50"):
                st.success("✅ ResNet50 loaded successfully!")
            else:
                st.error("❌ Failed to load ResNet50. Please try again.")

def about_chexpert_model_page():
    st.title("🧠 About CheXpert Model")

    st.markdown(
        """
        ## CheXpert: Stanford's Chest X-ray Interpretation Model

        **CheXpert** is a deep learning model for chest X-ray interpretation developed at Stanford University.
        It was trained on the CheXpert dataset, one of the largest publicly available chest X-ray datasets.

        ### The CheXpert Dataset

        - 224,316 chest radiographs from 65,240 patients
        - Collected from Stanford Hospital between 2002-2017
        - Labels extracted from radiology reports using natural language processing
        - 14 common chest radiographic observations including support devices

        ### Model Design

        The CheXpert model implemented here via TorchXRayVision:

        - Uses a DenseNet121 backbone architecture
        - Trained specifically on chest radiographs (not transferred from natural images)
        - Handles the uncertainty in radiology reports with a specialized labeling policy
        - Optimized for multi-label classification of 14 findings

        ### Unique Training Approach

        CheXpert uses an innovative approach to handle uncertain labels:

        - Labels are extracted from reports as **positive**, **negative**, or **uncertain**
        - Uncertain labels are treated with a special policy (either ignored, treated as negative, or treated as positive)
        - This approach addresses the inherent uncertainty in radiological reports

        ### Performance Benchmarks

        In the original study, CheXpert achieved:

        - Radiologist-level performance on 5 pathologies: atelectasis, cardiomegaly, consolidation, edema, and pleural effusion
        - ROC-AUC > 0.90 for cardiomegaly and edema
        - ROC-AUC > 0.85 for pleural effusion and consolidation

        ### Integration Details

        This implementation uses:

        - Pre-trained weights from TorchXRayVision library
        - DenseNet121 architecture with 224×224 resolution
        - Standard chest X-ray preprocessing with normalization
        - No further fine-tuning required

        ### Research Citation

        > Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., Marklund, H., Haghgoo, B., Ball, R., Shpanskaya, K., Seekins, J., Mong, D.A., Halabi, S.S., Sandberg, J.K., Jones, R., Larson, D.B., Langlotz, C.P., Patel, B.N., Lungren, M.P., & Ng, A.Y. (2019). CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison. Proceedings of the AAAI Conference on Artificial Intelligence, 33, 590-597.
        """
    )

    # Display information about pathologies
    st.markdown("### Pathologies Detected")

    pathologies = xrv.datasets.default_pathologies

    # Create a more visual display of pathologies
    num_columns = 3
    col_list = st.columns(num_columns)

    for i, pathology in enumerate(pathologies):
        col_index = i % num_columns
        with col_list[col_index]:
            st.markdown(f"✓ **{pathology}**")

    # Add a load button
    if st.button("Load CheXpert Model"):
        with st.spinner("Loading CheXpert..."):
            if load_model("CheXpert"):
                st.success("✅ CheXpert model loaded successfully!")
            else:
                st.error("❌ Failed to load CheXpert model. Please try again.")

def about_mimic_model_page():
    st.title("🧠 About MIMIC-CXR Model")

    st.markdown(
        """
        ## MIMIC-CXR: MIT's Chest X-ray Model for Diverse Populations

        **MIMIC-CXR** is a deep learning model trained on the MIMIC-CXR dataset, a large publicly available
        dataset of chest X-rays from the Beth Israel Deaconess Medical Center in Boston, collected from 2011-2016.

        ### The MIMIC-CXR Dataset

        - Over 377,000 chest X-rays from more than 65,000 patients
        - Demographically diverse patient population from urban hospital setting
        - Free-text radiology reports with structured labels
        - Developed by MIT Lab for Computational Physiology

        ### Model Architecture

        The MIMIC-CXR model implemented via TorchXRayVision:

        - DenseNet121 backbone architecture
        - Native training on chest X-rays (not transferred from natural images)
        - 224×224 pixel input resolution
        - Multi-label classification across 14 common findings

        ### Training Process and Optimization

        - Labels extracted via the CheXpert labeler for consistency
        - Trained using a weighted binary cross-entropy loss
        - Optimization via Adam optimizer with learning rate scheduling
        - Data augmentation to improve generalization

        ### Clinical Relevance

        MIMIC-CXR is particularly valuable for:

        - Applications requiring good performance across diverse demographics
        - Clinical settings similar to urban hospital environments
        - Research on algorithmic fairness in medical AI
        - Longitudinal patient studies (dataset includes multiple studies per patient)

        ### Model Advantages

        - Strong performance across diverse patient demographics
        - Good generalization to external datasets
        - Public availability through TorchXRayVision
        - Consistent labeling with CheXpert for comparability

        ### Research Citation

        > Johnson, A., Pollard, T., Mark, R., Berkowitz, S., & Horng, S. (2019). MIMIC-CXR Database (version 2.0.0). PhysioNet. https://doi.org/10.13026/C2JT1Q.

        > Johnson, A.E.W., Pollard, T.J., Berkowitz, S.J. et al. MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. Sci Data 6, 317 (2019). https://doi.org/10.1038/s41597-019-0322-0
        """
    )

    # Display information about pathologies
    st.markdown("### Pathologies Detected")

    pathologies = xrv.datasets.default_pathologies

    # Create a more visual display of pathologies
    num_columns = 3
    col_list = st.columns(num_columns)

    for i, pathology in enumerate(pathologies):
        col_index = i % num_columns
        with col_list[col_index]:
            st.markdown(f"✓ **{pathology}**")

    # Add a load button
    if st.button("Load MIMIC-CXR Model"):
        with st.spinner("Loading MIMIC-CXR..."):
            if load_model("MIMIC-CXR"):
                st.success("✅ MIMIC-CXR model loaded successfully!")
            else:
                st.error("❌ Failed to load MIMIC-CXR model. Please try again.")

def importance_gender_bias_page():
    st.title("📚 The Importance of Gender Bias")

    st.markdown(
        """
        ## Why Addressing Gender Bias in Medical AI Matters

        Gender bias in AI systems used for medical diagnosis can have significant real-world impacts on patient care and outcomes.

        ### Ethical Imperatives

        - **Fairness and Justice**: Healthcare systems should provide equal quality of care to all patients regardless of gender
        - **Non-maleficence**: AI systems should not cause harm by systematically misdiagnosing particular demographic groups
        - **Transparency**: Users of AI systems should understand potential biases and limitations

        ### Clinical Impact

        - **Diagnostic Disparities**: Biased models may miss diseases that present differently across genders
        - **Treatment Delays**: False negatives can lead to delayed treatment for vulnerable populations
        - **Resource Allocation**: Biased prioritization can affect access to limited healthcare resources

        ### Regulatory Requirements

        - **FDA Guidance**: Emerging regulations on AI in healthcare require fairness considerations
        - **Legal Liability**: Healthcare institutions may face liability for deploying biased systems
        - **Equal Treatment Mandates**: Many jurisdictions legally require equitable healthcare provision
        """
    )

    st.markdown("### Key Research on Gender Bias in AI Healthcare")

    research = [
        {
            "title": "A survey on bias and fairness in machine learning",
            "authors": "Mehrabi et al. (2021)",
            "findings": "Comprehensive review of various types of biases that can affect ML systems, including those in healthcare"
        },
        {
            "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
            "authors": "Obermeyer et al. (2019)",
            "findings": "Found that a widely used algorithm exhibited significant racial bias, underestimating the needs of Black patients"
        },
        {
            "title": "Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis",
            "authors": "Larrazabal et al. (2020)",
            "findings": "Demonstrated that gender imbalances in training data lead to significant performance disparities by gender"
        }
    ]

    for i, paper in enumerate(research):
        with st.expander(f"{i+1}. {paper['title']} ({paper['authors']})"):
            st.markdown(f"**Key Findings:** {paper['findings']}")

    st.markdown("### Mitigation Strategies")

    st.markdown(
        """
        Several approaches can help address gender bias in medical AI:

        1. **Data Representation**: Ensure diverse and balanced training datasets
        2. **Algorithmic Techniques**: Apply technical bias mitigation methods (as demonstrated in this app)
        3. **Evaluation Protocols**: Test models across demographic subgroups before deployment
        4. **Continuous Monitoring**: Track performance disparities in real-world usage
        5. **Stakeholder Inclusion**: Involve diverse clinicians and patients in development
        """
    )

def project_overview_page():
    st.title("📈 Project Overview")

    st.markdown(
        """
        ## Project Objective

        This application is designed to help healthcare organizations and AI developers detect and mitigate gender bias in chest X-ray interpretation models. Our goal is to ensure that AI tools in radiology deliver equitable performance across all patient demographics.

        ### Key Features

        1. **Data Exploration**: Upload and analyze medical imaging datasets
        2. **Multiple AI Models**: Compare performance across state-of-the-art chest X-ray models
        3. **Bias Detection**: Quantify gender disparities in model predictions
        4. **Mitigation Techniques**: Apply and evaluate various bias mitigation approaches
        5. **Explainable Analysis**: Understand the patterns behind model errors and bias

        ### Impact

        - **Clinical Safety**: Reduce misdiagnosis risks for underrepresented groups
        - **Regulatory Compliance**: Meet emerging requirements for AI fairness
        - **Improved Accessibility**: Make advanced AI diagnostics safe for all patients
        """
    )

    st.markdown("### Project Workflow")

    workflow_steps = [
        "Upload and analyze dataset",
        "Explore data characteristics and demographics",
        "Select and run AI models for disease prediction",
        "Analyze gender bias in model predictions",
        "Apply bias mitigation techniques",
        "Test and validate mitigation effectiveness"
    ]

    for i, step in enumerate(workflow_steps):
        st.markdown(f"**Step {i+1}:** {step}")

def meet_the_team_page():
    st.title("👥 Meet the Team")

    team_members = [
        {"name": "Yuying", "role": "Data Scientist", "image": "Yuying.webp"},
        {"name": "Siwen", "role": "ML Engineer", "image": "Siwen.webp"},
        {"name": "Zhi", "role": "Research Analyst", "image": "Zhi.webp"},
        {"name": "Maude", "role": "UX Designer", "image": "Maude.webp"}
    ]

    # Create a tab for each team member
    tabs = st.tabs([member["name"] for member in team_members])

    for tab, member in zip(tabs, team_members):
        with tab:
            # Try to display image if available
            try:
                if os.path.exists(member["image"]):
                    st.image(member["image"], width=150)
                else:
                    st.error(f"Image not found: {member['image']}")
            except:
                st.info(f"Image not available for {member['name']}")

            st.write(f"**{member['name']}**")
            st.write(f"*{member['role']}*")

# Main sidebar navigation
def main():
    st.sidebar.title("Gender Bias in Radiology")

    page_options = [
        "🏠 Home",
        "📂 Upload Data",
        "📊 Explore Data & Prepare",
        "🤖 Model Prediction",
        "⚖️ Gender Bias Analysis",
        "🛠️ Bias Mitigation & Simulation",
        "🧪 Gender Bias Testing",
        "🔍 Explainable Analysis",
        "🧠 About DenseNet121 Model",
        "🧠 About ResNet50 Model",
        "🧠 About CheXpert Model",
        "🧠 About MIMIC-CXR Model",
        "📚 The Importance of Gender Bias",
        "📈 Project Overview",
        "👥 Meet the Team"
    ]

    selected_page = st.sidebar.radio("Navigate", page_options)

    # Display selected page
    if selected_page == "🏠 Home":
        home_page()
    elif selected_page == "📂 Upload Data":
        upload_data_page()
    elif selected_page == "📊 Explore Data & Prepare":
        explore_data_page()
    elif selected_page == "🤖 Model Prediction":
        model_prediction_page()
    elif selected_page == "⚖️ Gender Bias Analysis":
        gender_bias_analysis_page()
    elif selected_page == "🛠️ Bias Mitigation & Simulation":
        bias_mitigation_simulation_page()
    elif selected_page == "🧪 Gender Bias Testing":
        gender_bias_testing_page()
    elif selected_page == "🔍 Explainable Analysis":
        explainable_analysis_page()
    elif selected_page == "🧠 About DenseNet121 Model":
        about_densenet_model_page()
    elif selected_page == "🧠 About ResNet50 Model":
        about_resnet_model_page()
    elif selected_page == "🧠 About CheXpert Model":
        about_chexpert_model_page()
    elif selected_page == "🧠 About MIMIC-CXR Model":
        about_mimic_model_page()
    elif selected_page == "📚 The Importance of Gender Bias":
        importance_gender_bias_page()
    elif selected_page == "📈 Project Overview":
        project_overview_page()
    elif selected_page == "👥 Meet the Team":
        meet_the_team_page()

    # Add app info in sidebar
    st.sidebar.markdown("---")
    st.sidebar.markdown("### About the App")
    st.sidebar.info(
        """
        **Gender Bias in Radiology**
        Version 2.0

        An application for detecting and mitigating
        gender bias in AI-driven chest X-ray analysis.
        """
    )

    # Version and credits
    st.sidebar.markdown("### Model Credits")
    st.sidebar.markdown("""
    Models provided by:
    - [TorchXRayVision](https://github.com/mlmed/torchxrayvision)
    - [PyTorch](https://pytorch.org/hub/)
    """)

if __name__ == "__main__":
    main()